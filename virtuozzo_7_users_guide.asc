Virtuozzo 7 Beta 1 User's Guide
===============================
:doctype: book

Learning Virtuozzo 7 Beta Basics
--------------------------------

This chapter provides a brief description of Virtuozzo 7 beta, containers, their specifications and underlying technologies.

Virtuozzo 7 Beta Overview
~~~~~~~~~~~~~~~~~~~~~~~~~

Virtuozzo 7 beta allows you to simultaneously run multiple containers on a single physical server. Using this software, you can efficiently use your server's hardware resources by sharing them among containers.

Virtuozzo provides the best value for cost conscious organizations enabling them to:

[options="compact"]
* standardize server hardware platforms,
* effectively consolidate server resources,
* consolidate and support legacy OSs and applications,
* streamline server and application deployment, maintenance, and management,
* simplify software testing and development,
* optimize server and application availability.

OS Virtualization Layer
~~~~~~~~~~~~~~~~~~~~~~~

This section provides detailed information on the OS virtualization layer responsible for providing support for Virtuozzo containers.

Basics of OS Virtualization
^^^^^^^^^^^^^^^^^^^^^^^^^^^

The OS virtualization allows you to virtualize physical servers on the operating system (kernel) layer. The diagram below shows the basic architecture of OS virtualization.

image::images/osvirtualization.png[align="center"]

The OS virtualization layer ensures isolation and security of resources between different containers. The virtualization layer makes each container appear as a standalone server. Finally, the container itself houses its own applications and workload. OS virtualization is streamlined for the best performance, management, and efficiency. Its main advantages are the following:

[options="compact"]
* Containers perform at levels consistent with native servers. containers have no virtualized hardware and use native hardware and software drivers making its performance unbeatable.
* Each container can seamlessly scale up to the resources of an entire physical server.
* OS virtualization technology provides the highest density available from a virtualization solution. You can create and run hundreds of containers on a standard production physical server.
* Containers use a single OS, making it extremely simple to maintain and update across containers. Applications may also be deployed as a single instance.

Virtuozzo Containers
^^^^^^^^^^^^^^^^^^^^

From the point of view of applications and container users, each container is an independent system. This independence is provided by the Virtuozzo OS virtualization layer. Note that only a negligible part of the CPU resources is spent on virtualization (around 1-2%). The main features of the virtualization layer implemented in Virtuozzo are the following:

[options="compact"]
* A container looks like a normal Linux system. It has standard startup scripts; software from vendors can run inside containers without any modifications or adjustment.
* A user can change any configuration file and install additional software inside containers.
* Containers are fully isolated from each other (file system, processes, sysctl variables).
* Containers share dynamic libraries, which greatly saves memory.
* Processes belonging to a container are scheduled for execution on all available CPUs. Consequently, containers are not bound to only one CPU and can use all available CPU power.

The two key parts of any container are the contents and configuration. By default, all container files are stored in the +/vz/private/_&lt;UUID&gt;_+ directory on the hardware node, also called _private area_.

.Key Container directories and files
[options="header",cols="1a,3a"]
|====
|File Name|Description

|+/vz/private/_&lt;UUID&gt;_+|Container private area.
|+/vz/private/_&lt;UUID&gt;_/root.hdd/root.hdd+|Virtual hard disk with container contents. The maximum size of the virtual hard disk is 16 TB.
|+/vz/root/_&lt;UUID&gt;_+|Container mount point.
|`ve.conf`|Container configuration file:

[options="compact"]
* Is symlinked to +/etc/vz/conf/_&lt;UUID&gt;_.conf+
* Defines container parameters, such as allocated resource limits, IP address and hostname, and so on.
* Overrides matching parameters in the global configuration file.
|====

All container files are stored in a single image (+/vz/private/_&lt;UUID&gt;_/root.hdd/root.hdd+). Such standalone nature:

[options="compact"]
* Enables easier migrations and backups due to a faster sequential I/O access to container images than to separate container files.
* Removes the need for OS and application templates once a container is created.
* Allows the use of native Linux disk quotas that are journaled and does not require quota recalculation after disasters like server crashes.

[NOTE]
[subs="quotes"]
====
*Note:* Using containers that store all files in an image file (also known as containers with the _container-in-an-image-file_ layout) is supported only for `/vz` partitions formatted as ext4.
====

Templates
^^^^^^^^^

A template (or a package set) in Virtuozzo is a set of original application files repackaged for use by Virtuozzo. Usually, it is just a set of RPM packages for Red Hat like systems. Virtuozzo provides tools for creating templates, installing, upgrading, adding them to and removing them from a container.

Using templates lets you:

[options="compact"]
* Share RAM among similar applications running in different containers to save hundreds of megabytes of memory.
* Deploy applications simultaneously in many containers.
* Use different versions of an application in different containers (for example, perform upgrades only in certain containers).

There are two types of templates: OS and application:

[options="compact"]
* An OS template is an operating system and the standard set of applications to be found right after the installation. Virtuozzo uses OS templates to create new containers with a preinstalled operating system.
* An application template is a set of repackaged software packages optionally accompanied with configuration scripts. Application templates are used to add extra software to existing containers.

For example, you can create a container on the basis of the `redhat` OS template and add the MySQL application to it with the help of the `mysql` template.

Virtuozzo Configuration
~~~~~~~~~~~~~~~~~~~~~~~

Virtuozzo allows you to configure settings for the physical server in general and for each container in particular. Among these settings are disk and user quotas, network parameters, default file locations, sample configuration files, and other.

Virtuozzo stores all OS virtualization-related configuration information in the global configuration file `/etc/vz/vz.conf`. It defines container parameters like the default OS templates, disk quotas, logging, and so on.

The configuration file is read when the Virtuozzo software and/or containers are started. However, many settings can also be changed on the fly by means of Virtuozzo standard utilities like `prlctl`, with or without modifying the corresponding configuration file to keep the changes for the future.

Resource Management
~~~~~~~~~~~~~~~~~~~

Virtuozzo resource management controls the amount of resources available to containers. The controlled resources include such parameters as CPU power, disk space, a set of memory-related parameters. Resource management allows you to:

[options="compact"]
* effectively share available physical server resources among containers
* guarantee Quality-of-Service in accordance with a service level agreement (SLA)
* provide performance and resource isolation and protect from denial-of-service attacks
* simultaneously assign and control resources for a number of containers
* collect usage information for system health monitoring

Resource management is much more important for Virtuozzo than for a standalone server since server resource utilization in such a system is considerably higher than that in a typical system.

Physical Server Availability Considerations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The availability of a physical server running Virtuozzo is more critical than the availability of a typical PC server. Since it runs multiple containers providing a number of critical services, physical server outage might be very costly. It can be as disastrous as the simultaneous outage of a number of servers running critical services.

To increase physical server availability, we suggest that you follow the recommendations below:

[options="compact"]
* Use a RAID storage for critical containers. Do prefer hardware RAIDs, but software mirroring RAIDs might suit too as a last resort.
* Do not run any software on the server itself. Create special containers where you can host necessary services such as BIND, FTPD, HTTPD, and so on. On the server, you need only the SSH daemon. Preferably, it should accept connections from a pre-defined set of IP addresses only.
* Do not create users on the server itself. You can create as many users as you need in any container. Remember: compromising the server means compromising all containers as well.

Managing Containers
-------------------

This chapter describes how to perform day-to-day operations on containers.

Creating Containers
~~~~~~~~~~~~~~~~~~~

To create a container, use the `prlctl create` command as follows:

[subs="quotes"]
----
*# prlctl create MyCT1 --vmtype ct*
----

Virtuozzo will create a new container with the name MyCT1 using the default parameters from the global configuration file `/etc/vz/vz.conf`, including the operating system that will be installed in the container. All container contents will be stored in this container's private area. To find out where the private area is located, use the `prlctl list` command as follows:

[subs="quotes"]
----
*# prlctl list MyCT1 -i | grep "Home"*
Home: /vz/private/26bc47f6-353f-444b-bc35-b634a88dbbcc
----

[NOTE]
[subs="quotes"]
====
*Notes:*
[options="compact"]
. The first time you install an operating system in a container, its cache is created. To create a cache, you need to have an active Internet connection to access repositories that contain packages for the respective operating system. You can also set up a local package repository and use this repository to provide packages for your operating system. A local package repository is also required for some commercial distributions (e.g., for Red Hat Enterprise Linux).
. For information on creating containers with preinstalled applications, see <<_using_os_template_caches_with_preinstalled_application_templates>>.
====

Supported Guest Operating Systems
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Listed below are the operating systems that have been tested in containers and are officially supported in Virtuozzo 7 beta.

[options="compact"]
* CentOS 7 (x86_64)
* CentOS 6 (x86_64)
* Ubuntu 14.04 LTS (x86_64)
* Debian 8 (x86_64)

Choosing an OS EZ Template
^^^^^^^^^^^^^^^^^^^^^^^^^^

Before starting to create a container, you shall decide on which OS EZ template your container will be based. There might be several OS EZ templates installed on the Hardware Node and prepared for the container creation; use the `vzpkg list` command to find out what OS EZ templates are available on your system:

[subs="quotes"]
----
*# vzpkg list -O*
centos-6-x86                       
centos-6-x86_64                    2012-05-10 13:16:43
----

Using the `-O` option with the `vzpkg list` command, you can list only the OS EZ templates installed on the Hardware Node. The time next to an OS EZ template indicates when the template was cached.

You can also use the `--with-summary` option to display brief information on the installed OS EZ templates:

[subs="quotes"]
----
*# vzpkg list -O --with-summary*
centos-6-x86                       :CentOS 6 EZ OS template
centos-6-x86_64                    :CentOS 6 (for AMD64/Intel EM64T) EZ OS Template
----

Performing Initial Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before starting your newly created container, you first need to configure it. This section describes the main configuration steps.

Configuring Network Settings
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To make containers accessible from the network, you need to assign valid IP addresses to them and configure DNS servers. The session below illustrates setting these parameters for container MyCT1:

* Assigning IPv4 and IPv6 addresses:
+
[subs="quotes"]
----
*# prlctl set MyCT1 --ipadd 10.0.186.101/24*
*# prlctl set MyCT1 --ipadd fe80::20c:29ff:fe01:fb08*
----
* Setting DNS server addresses:
+
[subs="quotes"]
----
*# prlctl set MyCT1 --nameserver 192.168.1.165*
----
+
[NOTE]
[subs="quotes"]
====
*Note:* To assign network masks to containers operating in the `venet0` network mode, you must set the `USE_VENET_MASK` parameter in the `/etc/vz/vz.conf` configuration file to `yes`.
====

Setting Passwords for Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In Virtuozzo, you can use the `--userpasswd` option of the `prlctl set` command to create new accounts in your containers directly from the hardware node. The created account can then be used to log in to the container. The easiest way of doing it is to run this command:

[subs="quotes"]
----
*# prlctl set MyCT1 --userpasswd user1:2wsx123qwe*
----

This command creates the `user1` account in the container MyCT1 and sets the `2wsx123qwe` password for it. Now you can log in to the container as `user1` and administer it in the same way you would administer a standalone server: install additional software, add users, set up services, and so on.

The `prlctl set` command can also be used to change passwords for existing accounts in your containers. For example, to change the password for `user1` in the container MyCT1 to `0pi65jh9`, run this command:

[subs="quotes"]
----
*# prlctl set MyCT1 --userpasswd user1:0pi65jh9*
----

When setting passwords for containers, keep in mind the following:

[options="compact"]
* You should use passwords that meet the minimum length and complexity requirements of the respective operating system. For example, for Windows Server 2008, a password must be more than six characters in length and contain characters from three of the following categories: uppercase characters, lowercase characters, digits, and non-alphabetic characters.
* You should not create accounts with empty passwords for containers running Linux operating systems.

Setting Startup Parameters
^^^^^^^^^^^^^^^^^^^^^^^^^^

The `prlctl set` command allows you to define the `onboot` startup parameter for containers. Setting this parameter to `yes` makes your container automatically boot at the physical server startup. For example, to enable container MyCT1 to automatically start on your server boot, you can execute the following command:

[subs="quotes"]
----
*# prlctl set MyCT1 --onboot yes*
----

Notice that the `onboot` parameter will have effect only on the next server startup.

Starting, Stopping, Restarting, and Querying Status of Containers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

After a container has been created, it can be managed like a usual computer.

Starting Containers
^^^^^^^^^^^^^^^^^^^

You can start containers with the `prlctl start` command. For example:

[subs="quotes"]
----
*# prlctl start MyCT1*
----

Stopping Containers
^^^^^^^^^^^^^^^^^^^

You can stop containers with the `prlctl stop` command. For example:

[subs="quotes"]
----
*# prlctl stop MyCT1*
----

Restarting Containers
^^^^^^^^^^^^^^^^^^^

You can restart containers with the `prlctl restart` command. For example:

[subs="quotes"]
----
*# prlctl restart MyCT1*
----

Checking Status of Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can check the status of a container with the `prlctl status` command. For example:

[subs="quotes"]
----
*# prlctl status MyCT1*
CT MyCT1 exists running
----

Listing Containers
~~~~~~~~~~~~~~~~~~

To get an overview of the containers existing on the physical server and to get additional information about them--their IP addresses, hostnames, current resource consumption, and so on--use the `prlctl list` command. In the most general case, you can get a list of all containers by issuing the following command:

[subs="quotes"]
----
*# prlctl list -a*
UUID                                    STATUS    IP_ADDR        T   NAME
{600adc12-0e39-41b3-bf05-c59b7d26dd73}  running   10.10.1.101    CT  MyCT1
----

The `-a` option tells the `prlctl list` command to output both running and stopped containers. By default, only running containers are shown. The default columns inform you of container UUID, status, IP address, type, and name. This output can be customized as desired by using `prlctl list` command line options. For example:

[subs="quotes"]
----
*# prlctl list -a -o name,ctid*
NAME                             UUID
MyCT1                            {26bc47f6-353f-444b-bc35-b634a88dbbcc}
----

This command displays only the names of existing containers and the IP addresses assigned to them.

Copying Containers within Server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can create a complete copy of a particular container (in respect of all the container data and resources parameters), or a _clone_. This saves your time because you do not have to think of setting up the container configuration parameters and the like. Moreover, you can create a number of clones at a sitting.

In Virtuozzo, you can use the `prlctl clone` command to copy a container within the given physical server. To clone a container, you need to stop it first. For example, you can create a clone of container MyCT1 by running:

[subs="quotes"]
----
*# prlctl clone MyCT1 --name 111*
----

The `--name` option specifies which name should be assigned to the new clone.

Checking the Cloned Container
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To check that your container has been successfully moved, run this command:

[subs="quotes"]
----
*# prlctl list -a*
UUID            STATUS       IP_ADDR        T  NAME
{62951c2a-...}  stopped      10.0.10.101    CT MyCT1
{49b66605-...}  stopped      10.0.10.101    CT MyCT2
----

As you can see from the example above, container MyCT2, the clone of container MyCT1, has been successfully created. However, before starting to use it, you should assign a different IP addresses to it which is currently identical to that of container MyCT1. Refer to <<_performing_initial_configuration>> to learn how you can do it.

Configuring Default Directories
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When cloning a container, you can also override the default directory +/vz/private/_&lt;dest_UUID&gt;_+ defining the container private area (where _&lt;dest_UUID&gt;_ denotes the UUID of the resulting container). To define a custom private area path for container 111, you can execute the following command:

[subs="quotes"]
----
*# prlctl clone MyCT1 --name 111 --dst /vz/private/customCTs*
----

Suspending Containers
~~~~~~~~~~~~~~~~~~~~~

Virtuozzo allows you to suspend a running container on the physical server by saving its current state to a special file. Later on, you can resume the container and get it in the same state the container was at the time of its suspending. Suspending your containers may prove useful, for example, if you need to restart the physical server, but do not want to:

[options="compact"]
* quit the applications currently running in the container
* spend much time on shutting down the guest operating system and then starting it again

You can use the `prlctl suspend` command to save the current state of a container . For example, you can issue the following command to suspend the container MyCT1:

[subs="quotes"]
----
*# prlctl suspend MyCT1*
----

At any time, you can resume the container MyCT1 by executing the following command:

[subs="quotes"]
----
*# prlctl resume MyCT1*
----

Once the restoration process is complete, any applications that were running in the container MyCT1 at the time of its suspending will be running again and the information content will be the same as it was when the container was suspended.

Running Commands in Containers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Virtuozzo allows you to execute arbitrary commands inside containers by running them on the physical server, i.e. without the need to log in to the respective container. For example, this can be useful in these cases:

[options="compact"]
* If you do not know the container login information, but need to run some diagnosis commands to verify that it is operational.
* If container has no network access.

In both cases, you can use the `prlctl exec` command to run a command inside the respective container. The session below shows how to run a stopped SSH daemon inside the container MyCT1:

[subs="quotes"]
----
*# prlctl exec MyCT1 /etc/init.d/sshd status*
sshd is stopped
*# prlctl exec MyCT1 /etc/init.d/sshd start*
Starting sshd:[OK]
*# prlctl exec MyCT1 /etc/init.d/sshd status*
sshd (pid 26187) is running...
----

[NOTE]
[subs="quotes"]
====
*Note:* The `prlctl exec` command is executed inside a container from the `/` directory rather than from `/root`.
====

Deleting Containers
~~~~~~~~~~~~~~~~~~~

You can delete a container that is not needed anymore using the `prlctl delete` command. Note that you cannot delete a running or mounted container. The example below illustrates deleting the running container MyCT1:

[subs="quotes"]
----
*# prlctl delete MyCT1*
Failed to remove the CT: Unable to complete the operation. This operation cannot be completed because the virtual environment "{4f27f27f-c056-4a65-abf6-27642b6edd21}" is in the "running" state.
*# prlctl stop MyCT1*
Stopping the CT...
The CT has been successfully stopped.
*# prlctl delete MyCT1*
Removing the CT...
The CT has been successfully removed.
----

Viewing Detailed Information About Containers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To view detailed information about a container, you can use the `prlctl list -i` command. For example:

[subs="quotes"]
----
*# prlctl list -i MyCT1*
----

Managing Templates
~~~~~~~~~~~~~~~~~~

A template in Virtuozzo is a pre-configured container that can be easily and quickly deployed into a fully functional container. Like any normal container, a template contains hardware (virtual disks, peripheral devices) and the operating system. It can also have additional software installed. In fact, the only main difference between a container and a template is that the latter cannot be started.

In Virtuozzo beta, you can perform the following operations on templates:

[options="compact"]
* create a new template
* list existing templates
* create a container from a template

These operations are described in the following subsections in detail.

Creating Templates
^^^^^^^^^^^^^^^^^^

In Virtuozzo, you can create a template using the `prlctl clone` utility. Making a template may prove useful if you need to create several containers with the same configuration. In this case, your steps can be as follows:

[options="compact"]
. You create a container with the required configuration.
. You make a template on the basis of the created container.
. You use the template to create as many containers as necessary.

Let us assume that you want to create a template of container MyCT1. To do this, you can run the following command:

[subs="quotes"]
----
*# prlctl clone MyCT1 --name template1 --template*
----

This command clones the container and saves it as the `template1` template. After the template has been successfully created, you can use it for creating new containers.

Listing Templates
^^^^^^^^^^^^^^^^^

Sometimes, you may need to get an overview of the templates available on your hardware node. For example, this may be necessary if you plan to create a container from a specific template, but do not remember its exact name. In this case, you can use the `prlctl list` command to list all templates on the hardware node and find the one you need:

[subs="quotes"]
----
*# prlctl list -t*
----

Deploying Templates
^^^^^^^^^^^^^^^^^^^

To convert a template into a container, use the `--ostemplate` option  of the `prlctl create` command. For example, to convert the `template1` template to a container MyCT2, you can run this command:

[subs="quotes"]
----
*# prlctl create MyCT2 --ostemplate template1*
----

To check that the container has been successfully created, use the `prlctl list -a` command:

[subs="quotes"]
----
*# prlctl list -a*
STATUS       IP_ADDR         NAME
running      10.12.12.101    MyCT3
stopped      10.12.12.34     MyCT2
----

The template itself is left intact and can be used for creating other containers:

[subs="quotes"]
----
*# prlctl list -t*
{64bd8fea-6047-45bb-a144-7d4bba49c849}  rhel            template1
----

Performing Container-specific Operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This section provides the description of operations specific to containers.

Reinstalling Containers
^^^^^^^^^^^^^^^^^^^^^^^

Reinstalling a container may help if any required container files have been inadvertently modified, replaced, or deleted, resulting in container malfunction. You can reinstall a container by using the `prlctl reinstall` command that creates a new container private area from scratch according to its configuration file and relevant OS and application templates. For example:

[subs="quotes"]
----
*# vzctl reinstall MyCT1*
----

[NOTE]
[subs="quotes"]
====
*Note:* If any of the container application templates cannot be added to the container in a normal way, reinstallation will fail. This may happen, for example, if an application template was added to the container using the `--force` option of the `vzpkgadd` or `vzpkg install` command.
====

To keep the personal data from the old container, the utility also copies the old private area contents to the +/vz/root/_&lt;UUID&gt;_/old+ directory of the new private area (unless the `--skipbackup` option is given). This directory may be deleted after you copy the personal data where you need.

The `prlctl reinstall` command retains user credentials base, unless the `--resetpwdb` option is specified.

Customizing Container Reinstallation
++++++++++++++++++++++++++++++++++++

The default reinstallation, as performed by the `prlctl reinstall` command, creates a new private area for the broken container as if it were created by the `prlctl create` command and copies the private area of the broken container to the `/old` directory in the new private area so that no file is lost. There is also a possibility of deleting the old private area altogether without copying or mounting it inside the new private area, which is done by means of the `--skipbackup` option. This way of reinstalling corrupted containers might in certain cases not correspond exactly to your particular needs. It happens when you are accustomed to creating new containers in some other way than just using the `prlctl create` command. For example, you may install additional software licenses into new containers, or anything else. In this case you would naturally like to perform reinstallation in such a way so that the broken container is reverted to its original state as determined by you, and not by the default behavior of the `prlctl create` command.

To customize reinstallation, you should write your own scripts determining what should be done with the container when it is being reinstalled, and what should be configured inside the container after it has been reinstalled. These scripts should be named `vps.reinstall` and `vps.configure`, respectively, and should be located in the `/etc/vz/conf` directory on the Hardware Node. To facilitate your task of creating customized scripts, the containers software is shipped with sample scripts that you may use as the basis of your own scripts.

When the +prlctl reinstall _&lt;UUID&gt;_+ command is called, it searches for the `vps.reinstall` and `vps.configure` scripts and launches them consecutively. When the `vps.reinstall` script is launched, the following parameters are passed to it:

[options="header",cols="1,3"]
|====
|Option|Description

|`--veid`|Container UUID.
|`--ve_private_tmp`|The path to the container temporary private area. This path designates where a new private area is temporarily created for the container. If the script runs successfully, this private area is mounted to the path of the original private area after the script has finished.
|`--ve_private`|The path to the container original private area.
|====

You may use these parameters within your `vps.reinstall` script.

If the `vps.reinstall` script finishes successfully, the container is started, and the `vps.configure` script is called. At this moment the old private area is mounted to the `/old` directory inside the new one irrespective of the `--skipbackup` option. This is done in order to let you use the necessary files from the old private area in your script, which is to be run inside the running container. For example, you might want to copy some files from there to regular container directories.

After the `vps.configure` script finishes, the old private area is either dismounted and deleted or remains mounted depending on whether the `--skipbackup` option was provided.

If you do not want to run these reinstallation scripts and want to stick to the default `prlctl reinstall` behavior, you may do either of the following:

[options="compact"]
* Remove the `vps.reinstall` and `vps.configure` scripts from the `/etc/vz/conf` directory, or at least rename them;
* Modify the last line of the `vps.reinstall` script so that it would read `exit 128` instead of `exit 0`.

The exit code `128` tells the utility not to run the scripts and to reinstall the container with the default behavior.

Enabling VPN for Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Virtual Private Network (VPN) is a technology which allows you to establish a secure network connection even over an insecure public network. Setting up a VPN for a separate container is possible via the TUN/TAP device. To allow a particular container to use this device, do the following:

. Make sure the `tun.o` module is already loaded before Virtuozzo is started:
+
[subs="quotes"]
----
*# lsmod*
----
. Allow the container to use the TUN/TAP device:
+
[subs="quotes"]
----
*# vzctl set MyCT1 --devices c:10:200:rw --save*
----
. Create the corresponding device inside the container and set the proper permissions:
+
[subs="quotes"]
----
*# prlctl exec MyCT1 mkdir -p /dev/net*
*# prlctl exec MyCT1 mknod /dev/net/tun c 10 200*
*# prlctl exec MyCT1 chmod 600 /dev/net/tun*
----

Configuring the VPN properly is a common Linux administration task, which is out of the scope of this guide. Some popular Linux software for setting up a VPN over the TUN/TAP driver includes http://vtun.sourceforge.net[Virtual TUNnel] and http://openvpn.sourceforge.net[OpenVPN].

Enabling NFSv4 Support for Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To enable support for NFSv4 mounts in containers, run the following command on the Hardware Node:

[subs="quotes"]
----
*# sysctl -w fs.nfs.nfs4_ct_enable=1*
----

To make sure the NFSv4 support remains enabled after Node reboot, add the line `fs.nfs.nfs4_ct_enable=1` to `/etc/sysctl.conf`.

Next time a resource is mounted, Virtuozzo will automatically choose the latest NFS version supported by both the server and client.

To see support for which versions of NFS is currently enabled, check `/proc/fs/nfsd/versions`. For example:

[subs="quotes"]
----
*# cat /proc/fs/nfsd/versions*
+2 +3 +4 -4.1
----

[NOTE]
[subs="quotes"]
====
*Note:* For this to work, you need to have started the `nfs` service on the Hardware Node at least once.
====

In our example, the support for NFS versions 2, 3, and 4 is enabled, while the support for NFS version 4.1 is disabled. To enable it:

. Run the following command:
+
[subs="quotes"]
----
*# echo "+4.1" &gt; /proc/fs/nfsd/versions*
----
. Restart the Hardware Node.

Setting Up NFS Server in Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To set up an NFS server in a container, do the following:

. Make sure the `rpcbind`, `nfsd`, and `nfslock` services are installed in the container.
. Enable the NFS server feature for the container by running the `vzctl set --features nfsd:on` command on the Hardware Node. For example:
+
[subs="quotes"]
----
*# vzctl set MyCT1 --feature nfsd:on --save*
----
+
[NOTE]
[subs="quotes"]
====
*Note:* If the container is running, restart it for the changes to take effect.
====
. Start the `rpcbind` service in the container.
+
[subs="quotes"]
----
*# service rpcbind start*
Starting rpcbind:                                          [  OK  ]
----
. Start the `nfs` and `nfslock` services in the container.
+
[subs="quotes"]
----
*# service nfs start*
Starting NFS services:                                     [  OK  ]
Starting NFS quotas:                                       [  OK  ]
Starting NFS mountd:                                       [  OK  ]
Starting NFS daemon:                                       [  OK  ]
*# service nfslock start*
Starting NFS statd:                                        [  OK  ]
----

You can now set up NFS shares in the configured container.

[NOTE]
[subs="quotes"]
====
*Note:* NFSv4 support may be disabled by default. For details on how to enable it, see <<_enabling_nfsv4_support_for_containers>>.
====

Mounting NFS Shares on Container Start
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If you configured an NFS share in the `/etc/fstab` file of a CentOS or RHEL-based container, and you need this NFS share to be mounted on container start, enable autostart for the `netfs` service with the `chkconfig netfs on` command.

Adding Multiple Virtual Disks to Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Even though new containers are created with just one virtual hard disk, you can add more disks to a container and keep the corresponding ploop images at locations of your choice, be it directly attached HDDs or SSDs or Virtuozzo storage. Such functionality allows creating more flexible containers, in which, for example, the operating system is kept on a fast SSD and user content is stored on a capacious HDD or Virtuozzo storage.

To add a virtual hard disk to a container, whether stopped or running, use the `prlctl set --device-add hdd` command. For example:

[subs="quotes"]
----
*# prlctl set MyCT1 --device-add hdd --image /hdd/MyCT1 --size 100G --mnt /userdisk*
----

This command adds to the configuration of container MyCT1 a virtual hard disk with the following parameters:

[options="compact"]
* name: +hdd__<N>__+ where _&lt;N&gt;_ is the next available disk index,
* image location: `/hdd/MyCT1`,
* size: 102400 MB,
* mount point inside container MyCT1: `/userdisk`. A corresponding entry is also added to container's `/etc/fstab` file.

Managing Resources
------------------

The main goal of resource control in Virtuozzo is to provide Service Level Management or Quality of Service for containers. Correctly configured resource control settings prevent serious impacts resulting from the resource over-usage (accidental or malicious) of any container on the other containers. Using resource control parameters for resource management also allows you to enforce fairness of resource usage among containers and better service quality for preferred containers, if necessary. All these parameters can be set using command-line utilities.

Managing CPU Resources
~~~~~~~~~~~~~~~~~~~~~~

In this Virtuozzo beta, you can manage the following CPU resource parameters for containers:

[options="compact"]
* CPU units for containers
* CPU affinity for containers
* CPU limits for containers
* NUMA nodes for containers

Detailed information on these parameters is given in the following sections.

Configuring CPU Units
^^^^^^^^^^^^^^^^^^^^^

CPU units define how much CPU time one container can receive in comparison with the other containers on the Hardware Node if all the CPUs of the Hardware Node are fully used. For example, if containers MyCT1 and MyCT3 are set to receive 1000 CPU units each and container MyCT2 is configured to get 2000 CPU units, container MyCT2 will get twice as much CPU time as containers MyCT1 or MyCT3 if all the CPUs of the Node are completely loaded.

By default, each container on the Node gets 1000 CPU units. You can configure the default setting using the `prlctl set` command. For example, you can run the following commands to allocate 2000 CPU units to container MyCT1:

[subs="quotes"]
----
*# prlctl set MyCT1 --cpuunits 2000*
----

Configuring CPU Affinity for Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If your physical server has several CPUs installed, you can bind a container to specific CPUs so that only these CPUs are used to handle the processes running in the container. The feature of binding certain processes to certain CPUs is known as _CPU affinity_. Establishing CPU affinity between containers and physical processors may help you increase your system performance by up to 20%.

By default, any newly created container can consume the CPU time of all processors installed on the physical server. To bind a container to specific CPUs, you can use the `--cpumask` option of the `prlctl set` command. Assuming that your physical server has 8 CPUs, you can make the processes in the container MyCT1 run on CPUs 0, 1, 3, 4, 5, and 6 by running the following commands:

[subs="quotes"]
----
*# prlctl set MyCT1 --cpumask 0,1,3,4-6*
----

You can specify the CPU affinity mask--that is, the processors to bind to containers--as separate CPU index numbers (0,1,3) or as CPU ranges (4-6). If you are setting the CPU affinity mask for a running container, the changes are applied on the fly.

To undo the changes made to the container MyCT1 and set their processes to run on all available CPUs on the server, run this command:

[subs="quotes"]
----
*# prlctl set MyCT1 --cpumask all*
----

Configuring CPU Limits for Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A _CPU limit_ indicates the maximum CPU power a container may get for its running processes. The container is not allowed to exceed the specified limit even if the server has enough free CPU power. By default, the CPU limit parameter is disabled for all newly created containers. This means that any application in any container can use all the free CPU power of the server.

To set a CPU limit for a container, you can use one of these options: `--cpulimit`, `--cpus`. Both options are described below in detail.

Using --cpulimit to Set CPU Limits
++++++++++++++++++++++++++++++++++

As a rule, you set a CPU limit for a container by using the `--cpulimit` option with the `prlctl set` command. In the following example, container MyCT1 is set to receive no more than 25% of the server CPU time even if the CPUs on the server are not fully loaded:

[subs="quotes"]
----
*# prlctl set MyCT1 --cpulimit 25*
----

This command sets the CPU limit for container MyCT1 to 25% of the total CPU power of the server. The total CPU power of a server in per cent is calculated by multiplying the number of logical CPU cores installed on the server by 100%. So if a server has 2 logical CPU cores, 2 GHz each, the total CPU power will equal 200% and the limit for container MyCT1 will be set to 500 MHz.

Now imagine the situation when you migrate container MyCT1 to another Hardware Node with 2 logical CPU cores, 3 GHz each. On this server, container MyCT1 will be able to get 25% of 6 GHz--that is, 750 MHz. To ensure that container MyCT1 always has the same CPU limit on all servers, irrespective of their total CPU power, you can set the CPU limits in megahertz (MHz). For example, to make container MyCT1 consume no more than 500 MHz on any Hardware Node, run the following command:

[subs="quotes"]
----
*# prlctl set MyCT1 --cpulimit 500m*
----

[NOTE]
[subs="quotes"]
====
*Note:* For more information on setting CPU limits for containers, see also <<_cpu_limit_specifics>>.
====

Using --cpus to set CPU limits
++++++++++++++++++++++++++++++

Another way of setting a CPU limit for a container is to use the `--cpus` option with the `prlctl set` command. You may want to use this command if your server has several CPU cores. In this case, you can specify the desired number of CPU cores after `--cpus` and the CPU limit will be set to the sum of CPU powers of the specified cores. For example, if a server has 4 CPU cores, 3 GHz each, you can allow container MyCT1 to consume no more than 6 GHz of CPU power by running this command:

[subs="quotes"]
----
*# prlctl set MyCT1 --cpus 2*
----

To check that the CPU limit has been successfully set, you can execute the following command:

[subs="quotes"]
----
*# vzlist -o cpulimitM MyCT1*
 CPUL_M
     6000
----

As you can see, the CPU limit for container MyCT1 is now set to 6000 MHz (or 6 GHz).

Along with setting the CPU limit for a container on the server, the `--cpus` option also defines how the information on available CPUs is shown to users. So in the example above, if you log in to container MyCT1 and run the `cat /proc/cpuinfo` command, you will see that only 2 CPUs are installed in the container:

[subs="quotes"]
----
*# prlctl exec MyCT1 cat /proc/cpuinfo*
processor       : 0
vendor_id       : GenuineIntel
cpu family      : 15
model           : 4
model name      : Intel(R) Xeon(TM) CPU 3.00GHz
stepping        : 1
cpu MHz         : 2993.581
cache size      : 1024 KB
...
processor       : 1
vendor_id       : GenuineIntel
cpu family      : 15
model           : 4
model name      : Intel(R) Xeon(TM) CPU 3.00GHz
stepping        : 1
cpu MHz         : 2993.581
cache size      : 1024 KB
...
----

Using --cpulimit And --cpus Simultaneously
++++++++++++++++++++++++++++++++++++++++++

If you use both `--cpulimit` and `--cpus` to set the CPU limit for a container, the smallest limit applies. For example, running the following commands on a server with 4 CPUs, 2 GHZ each, will set the limit for container MyCT1 to 2 GB:

[subs="quotes"]
----
*# prlctl set MyCT1 --cpus 2*
*# prlctl set MyCT1 --cpulimit 2000m*
----

CPU Limit Specifics
+++++++++++++++++++

Internally, Virtuozzo sets the CPU limit for containers in percent. On multi-core systems, each logical CPU core is considered to have the CPU power of 100%. So if a server has 4 CPU cores, the total CPU power of the server equals 400%.

You can also set a CPU limit in megahertz (MHz). If you specify the limit in MHz, Virtuozzo uses the following formula to convert the CPU power of the server from MHz into percent: `CPULIMIT_% = 100% * CPULIMIT_MHz / CPUFREQ`, where

[options="compact"]
* `CPULIMIT_%` is the total CPU power of the server in percent.
* `CPULIMIT_MHz` is the total CPU power of the server in megahertz.
* `CPUFREQ` is the CPU frequency of one core on the server.

When setting CPU limits, note the following:

[options="compact"]
* Make sure that the CPU limit you plan to set for a container does not exceed the total CPU power of the server. So if a server has 4 CPUs, 1000 MHz each, do not set the CPU limit to more than 4000 MHz.
* The processes running in a container are scheduled for execution on all server CPUs in equal shares. For example, if a server has 4 CPUs, 1000 MHz each, and you set the CPU limit for a container to 2000 MHz, the container will consume 500 MHz from each CPU.
* All running containers on a server cannot simultaneously consume more CPU power than is physically available on the node. In other words, if the total CPU power of the server is 4000 MHz, the running containers on this server will not be able to consume more than 4000 MHz, irrespective of their CPU limits. It is, however, perfectly normal that the overall CPU limit of all containers exceeds the Node total CPU power because most of the time containers consume only part of the CPU power assigned to them.

Binding CPUs to NUMA Nodes
^^^^^^^^^^^^^^^^^^^^^^^^^^

On systems with a NUMA (Non-Uniform Memory Access) architecture, you can configure containers to use CPUs from specific NUMA nodes only. Let us assume the following:

[options="compact"]
* Your physical server has 8 CPUs installed.
* The CPUs are divided into 2 NUMA nodes: NUMA node 0 and NUMA node 1. Each NUMA node has 4 CPUs.
* You want the  processes in container MyCT1 to be executed on the processors from NUMA node 1.

To set container MyCT1 to use the processors from NUMA node 1, run the following command:

[subs="quotes"]
----
*# vzctl set MyCT1 --nodemask 1 --save*
----

To check that container MyCT1 is now bound to NUMA node 1, use this command:

[subs="quotes"]
----
*# vzlist MyCT1 -o nodemask*
         NODEMASK
                1
----

To unbind container MyCT1 from NUMA node 1, execute this command:

[subs="quotes"]
----
*# vzctl set MyCT1 --nodemask all --save*
----

Now container MyCT1 should be able to use all CPUs on the server again.

[NOTE]
[subs="quotes"]
====
*Note:* For more information on NUMA, visit http://lse.sourceforge.net/numa.
====

Managing Disk Quotas
~~~~~~~~~~~~~~~~~~~~

This section explains the basics of disk quotas, describes disk quota parameters and how you can set them.

What are Disk Quotas?
^^^^^^^^^^^^^^^^^^^^^

In the current version of Virtuozzo, system administrators can limit the amount of disk space containers can use. Such quotas are known as per-container or first-level quotas. In addition, administrators can enable or disable per-user and per-group quotas that are known as second-level quotas and allow you to limit disk space that individual users and groups in a container can use.

By default, first-level quotas on your Hardware Node are enabled (as defined in the `/etc/vz/vz.conf` configuration file), whereas second-level quotas must be turned on for each container separately (in the corresponding container configuration files). It is impossible to turn on second-level disk quotas for a container if first-level disk quotas are off.

Disk Quota Parameters
^^^^^^^^^^^^^^^^^^^^^

The table below summarizes the disk quota parameters that you can control. The *File* column indicates that the parameter is defined in a container configuration file (V) or in the global configuration file but can be overridden in a container configuration file (GV).

[options="header",cols="2,3,1"]
|====
|Parameter|Description|File

|`DISK_QUOTA`|Enables or disables per-Container quotas for all or particular containers.|GV
|`DISKSPACE`|The total disk space a container may consume, in kilobytes.|V
|`QUOTAUGIDLIMIT`|Configures per-user and per-group quotas: enables if set to a value other than `0` and disables if set to `0`.|{nbsp}
|====

Managing Per-Container Disk Quotas
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This section explains how to manage per-container disk quotas.

Enabling and Disabling Per-Container Quotas
+++++++++++++++++++++++++++++++++++++++++++

Per-container disk quotas can be enabled or disabled with the `DISK_QUOTA` parameter in the global configuration file (`/etc/vz/vz.conf`). The same parameter in a container configuration file (+/etc/vz/conf/_&lt;UUID&gt;_.conf+) overrides the one in the global configuration file. To enable quota support for some containers and disable it for the rest, it is recommended to set `DISK_QUOTA` to `yes` in the global configuration file and then set it to `no` in the corresponding container configuration files.

Have in mind that for ploop-based containers, per-container quotas do not work, as container size is already limited by ploop image size. For such containers, the commands below show disk space available in the ploop image.

[NOTE]
[subs="quotes"]
====
*Note:* Setting the `DISK_QUOTA` parameter to `no` also disables second-level quotas for all containers.
====

In the example below, per-container quotas are enabled globally but disabled for container MyCT1:

. Check that quota is enabled.
+
[subs="quotes"]
----
*# grep DISK_QUOTA /etc/vz/vz.conf*
DISK_QUOTA=yes
----
. Check available space on the `/vz` partition.
+
[subs="quotes"]
----
*# df /vz*
Filesystem           1k-blocks      Used Available Use% Mounted on
/dev/sda2              8957295   1421982   7023242  17% /vz
----
. Set `DISK_QUOTA` to `no` in the container MyCT1 configuration file.
. Check that quotas are disabled for container MyCT1.
+
[subs="quotes"]
----
*# grep DISK_QUOTA /etc/vz/conf/26bc47f6-353f-444b-bc35-b634a88dbbcc.conf*
DISK_QUOTA=no
*# prlctl start MyCT1*
*# prlctl exec MyCT1 df*
Filesystem           1k-blocks      Used  Available  Use%  Mounted on
/dev/ploop1p1          8282373    747060    7023242   10%  /
----

Setting Quota Parameters
++++++++++++++++++++++++

To set disk quotas for containers, you use the `prlctl set` command. In the example below, the disk space available to container MyCT1 is set to 20 GB:

[subs="quotes"]
----
*# prlctl set MyCT1 --diskspace 20971520*
----

You can check the result as follows:

[subs="quotes"]
----
*# prlctl exec MyCT1 df*
Filesystem           1K-blocks      Used Available Use% Mounted on
/dev/ploop1p1         20642152    737692  18855888   4% /
none                    131072         4    131068   1% /dev
----

You can change the per-container disk quota parameters for running containers. The changes take effect immediately.

Managing Per-User and Per-Group Disk Quotas
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This section explains how to manage per-user and per-group disk quotas for containers.

Enabling and Disabling Per-User and Per-Group Quotas
++++++++++++++++++++++++++++++++++++++++++++++++++++

You can enable or disable per-user and per-group disk quotas for a container by using the `prlctl set --quotaugidlimit` command and then restarting the container. You can also disable quotas for all containers by setting the `DISK_QUOTA` parameter to `no` in the global configuration file (`/etc/vz/vz.conf`).

To enable quotas, use:

[subs="quotes"]
----
*# prlctl set MyCT1 --quotaugidlimit 1*
----

To disable quotas, use:

[subs="quotes"]
----
*# prlctl set MyCT1 --quotaugidlimit 0*
----

[NOTE]
[subs="quotes"]
====
*Note:* This command changes the `QUOTAUGIDLIMIT` parameter in the container configuration file (+/etc/vz/conf/_&lt;UUID&gt;_.conf+).
====

Managing Quota Parameters
+++++++++++++++++++++++++

Virtuozzo provides the standard Linux `quota` package for working inside containers:

[subs="quotes"]
----
*# prlctl exec MyCT1 rpm -q quota*
quota-3.17-16.el6.x86_64
----

This command shows that the `quota` package installed in the container is built and shipped by Odin Use the utilities from this package (as is prescribed in your Linux manual) to set second-level quotas for the given container. For example:

[subs="quotes"]
----
*# prlctl enter MyCT1*
*CT-4f27f27f /# edquota root*
Disk quotas for user root (uid 0):
  Filesystem      blocks        soft      hard     inodes     soft    hard
  /dev/ploop1p1    38216       50000     60000    45454      70000   70000
*CT-4f27f27f /# repquota -a*
\*\** Report for user quotas on device /dev/ploop1p1
Block grace time: 00:00; Inode grace time: 00:00
                        Block limits                File limits
User            used    soft    hard  grace    used  soft  hard  grace
 ----------------------------------------------------------------------
root      --   38218   50000   60000          45453 70000 70000
...
*CT-4f27f27f /# dd if=/dev/zero of=test*
dd: writing to 'test': Disk quota exceeded
23473+0 records in
23472+0 records out
*CT-4f27f27f /# repquota -a*
\*** Report for user quotas on device /dev/ploop1p1
Block grace time: 00:00; Inode grace time: 00:00
                        Block limits                File limits
User            used    soft    hard  grace    used  soft  hard  grace
 ----------------------------------------------------------------------
root      +-   50001   50000   60000   none   45454 70000 70000
...
----

The above example shows the session when the `root` user has the disk space quota set to the hard limit of 60,000 1KB blocks and to the soft limit of 50,000 1KB blocks; both hard and soft limits for the number of inodes are set to 70,000.

It is also possible to set the grace period separately for block limits and inodes limits with the help of the `/usr/sbin/setquota` command. For more information on using the utilities from the `quota` package, consult the system administration guide shipped with your Linux distribution or online manual pages included in the package.

Managing Virtual Disks
~~~~~~~~~~~~~~~~~~~~~~

In Virtuozzo, you can manage virtual disks as follows:

[options="compact"]
* increase the capacity of your virtual disks,
* reduce the capacity of your virtual disks,
* compact virtual disks (reduce the size they occupy on the physical hard drive).

All these operations are described in the following subsections in detail.

Increasing Disk Capacity
^^^^^^^^^^^^^^^^^^^^^^^^

If you find that the capacity of the virtual hard disk of your container does not fit your needs anymore, you can increase it using the `prlctl set --device-set` command. For example:

[subs="quotes"]
----
*# prlctl set MyCT1 --device-set hdd0 --size 80G*
----

When increasing the disk capacity, keep in mind the following:

[options="compact"]
* You cannot increase the capacity of a virtual disk if the container using this disk is running.
* The capacity of an expanding virtual disk shown from inside the container and the size the virtual disk occupies on the server's physical disk may differ.

Reducing Disk Capacity
^^^^^^^^^^^^^^^^^^^^^^

Virtuozzo provides a possibility to reduce the size of an expanding virtual disk by setting the limit the disk cannot exceed. To do this, use the `prlctl set --device-set` command. For example:

[subs="quotes"]
----
*# prlctl set MyCT1 --device-set hdd0 --size 30G*
----

When reducing the disk capacity, keep in mind the following:

[options="compact"]
* You cannot reduce the capacity of a virtual disk if the container using this disk is running.
* The capacity of an expanding virtual disk shown from inside the container and the size the virtual disk occupies on the server's physical disk may differ.
* You cannot reduce XFS filesystems on LVM (the default choice for CentOS 7 and Red Hat Enterprise Linux 7).

Checking the Minimum Disk Capacity
++++++++++++++++++++++++++++++++++

If, before reducing disk capacity, you want to know the minimum to which it can be reduced, use the `prl_disk_tool resize --info` command. For example:

[subs="quotes"]
----
*# prl_disk_tool resize --info --hdd /vz/private/26bc47f6-353f-444b-bc35-b634a88dbbcc/harddisk.hdd*
Disk information:
...
        Minimum:                                        2338M
...
----

Compacting Disks
^^^^^^^^^^^^^^^^

In Virtuozzo, you can decrease the space your containers occupy on the hardware node's disk drive by compacting their virtual disks. Compacting virtual disks allows you to save your server's disk space and host more containers on the server.

[NOTE]
[subs="quotes"]
====
*Note:* Plain disks cannot be compacted.
====

To compact a virtual disk, you can use the `prl_disk_tool compact` command. For example:

[subs="quotes"]
----
*# prl_disk_tool compact --hdd /vz/private/26bc47f6-353f-444b-bc35-b634a88dbbcc/harddisk.hdd/*
----

To check the space that was freed by compacting the virtual disk, you can use standard Linux utilities (for example, `df`).

Managing Network Accounting and Bandwidth
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This section explains how to perform the following tasks in Virtuozzo:

[options="compact"]
* configuring network classes
* viewing network traffic statistics
* turning on and off network bandwidth management
* configuring bandwidth limits

Network Traffic Parameters
^^^^^^^^^^^^^^^^^^^^^^^^^^

The table below summarizes the network traffic parameters that you can control in Virtuozzo.

[options="header",cols="1,3"]
|====
|Parameter|Description

|`traffic_shaping`|If set to `yes`, traffic limitations for outgoing traffic are set for containers. The default is `no`.
|`bandwidth`|This parameter lists all network adapters installed on the Hardware Node and their bandwidth.
|`totalrate`|This parameter defines the bandwidth to allocate for each network class. It is active if traffic shaping is turned on.
|`rate`|If traffic shaping is turned on, this parameter specifies the bandwidth guarantee for containers.
|`ratebound`|If this parameter is set to `yes`, the bandwidth guarantee (the global rate parameter) is also the limit for the container, and the container cannot borrow the bandwidth from the `totalrate` bandwidth pool.
|====

Configuring Network Classes
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Virtuozzo allows you to track the inbound and outbound network traffic as well as to shape the outgoing traffic for a container. To provide the ability to distinguish between domestic and international traffic, a concept of network classes is introduced. It is important to fully understand this notion, because network classes IDs are used in the values of some network traffic parameters. A network class is a range of IP addresses for which Virtuozzo counts and shapes the traffic.

Classes are specified in the `/etc/vz/conf/networks_classes` file. The file is in the ASCII format, and all empty lines and lines starting with the `#` sign are ignored. Other lines have the following format:

[subs="quotes"]
----
_&lt;class_id&gt; &lt;IP_address&gt;/&lt;prefix_length&gt;_
----

where +_&lt;class_id&gt;_+ defines the network class ID, and the +_&lt;IP_address&gt;/&lt;prefix_length&gt;_+ pair defines the range of IP addresses for this class. There may be several lines for each class.

Classes 0 and 1 have special meanings:

[options="compact"]
* Class 0 defines the IP address range for which no accounting is performed. Usually, it corresponds to the Hardware Node subnet (the Hardware Node itself and its containers). Setting up class 0 is not required; however, its correct setup improves performance.
* Class 1 is defined by Virtuozzo to match any IP address. It must be always present in the network classes definition file. Therefore, it is suggested not to change the default line in the `networks_classes` file.
+
----
1 0.0.0.0/0
----
+
If your containers are using IPv6 addresses, you can also add the following line to this file:
+
----
1 ::/0
----

Other classes should be defined after class 1. They represent exceptions from the "matching-everything" rule of class 1. The example below illustrates a possible configuration of the network classes definition file containing rules for both IPv4 and IPv6 addresses:

----
# Hardware Node networks
0 192.168.0.0/16
0 fe80::/64
# any IP address (all traffic)
1 0.0.0.0/0
1 ::/0
# class 2 - addresses for the "foreign" traffic
2 10.0.0.0/8
2 2001:db88::/64
# inside "foreign" network there
# is a hole belonging to "local" traffic
1 10.10.16.0/24
1 2001:db88:3333::/64
----

In this example, IPv4 addresses in the range of `192.168.0.0` to `192.168.255.255` and IPv6 addresses in the range of `fe80::` to `fe80::ffff:ffff:ffff:ffff` are treated as class 0 addresses and no accounting is done for the traffic from containers destined to these addresses.

Class 2 matches the following IP addresses:

[options="compact"]
* IPv4 addresses from `10.0.0.0` to `10.255.255.255` with the exception of addresses in the sub-range of `10.10.16.0` to `10.10.16.255`, which are treated as class 1.
* IPv6 addresses from `2001:db88::` to `2001:db88::ffff:ffff:ffff:ffff` with the exception of addresses in the sub-range of `2001:db88:3333::` to `2001:db88:3333::ffff:ffff:ffff:ffff`, which are also treated as class 1.

All other IP addresses (both IPv4 and IPv6) belong to class 1.

[NOTE]
[subs="quotes"]
====
*Note:* After editing the `/etc/vz/conf/networks_classes` file, execute either the `/etc/init.d/vz accrestart` or `service vz accrestart` command for the changes made to the file to take effect.
====

Viewing Network Traffic Statistics
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In Virtuozzo, you can view the current network traffic statistics for containers using the `vznetstat` utility. For example:

[subs="quotes"]
----
*# vznetstat*
UUID         Net.Class   Input(bytes) Input(pkts)    Output(bytes) Output(pkts)
0            0           270273925       77024          3619544       58405
d59da013-... 0                   0           0                0           0
da72a150-... 0                   0           0                0           0
26bc47f6-... 0                   0           0                0           0
----

By default, `vznetstat` shows network statistics for both containers. In the example above, this statistics is displayed for two containers with names MyCT1 and MyCT2. Keep in mind that the `vznetstat` utility displays statistics only about containers that were started at least once.

The `vznetstat` utility displays the following information:

[options="header",cols="1,3"]
|====
|Column|Description

|UUID|UUID assigned to container.
|Net.Class|ID of the network class for which network statistics is calculated.
|Input(bytes)|Amount of incoming traffic, in bytes.
|Input(pkts)|Amount of incoming traffic, in packets.
|Output(bytes)|Amount of outgoing traffic, in bytes.
|Output(pkts)|Amount of outgoing traffic, in packets.
|====

For example, from the command output above, you can see that around 58 MB of data were uploaded to container MyCT1, (2) about 10 MB were downloaded from it, and all the traffic was exchanged with servers from class 1 networks.

If necessary, you can view network traffic statistics separately for container by passing the `-t` option to `vznetstat`:

[subs="quotes"]
----
*# vznetstat -t ct*
UUID         Net.Class   Input(bytes) Input(pkts)    Output(bytes) Output(pkts)
0            0           270273925       77024          3619544       58405
d59da013-... 0                   0           0                0           0
da72a150-... 0                   0           0                0           0
26bc47f6-... 0                   0           0                0           0
----

You can also view network statistics for a particular container by specifying its ID after the `-v` option, for example:

[subs="quotes"]
----
*# vznetstat -v MyCT1*
UUID         Net.Class   Input(bytes) Input(pkts)    Output(bytes) Output(pkts)
d59da013-... 0                   0           0                0           0
----

This command displays statistics only for container with the UUID d59da013-{skipped}.

Enabling and Disabling Network Bandwidth Management
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Traffic shaping (also known as network bandwidth management) allows you to control what network bandwidth a container receives for outgoing traffic. Traffic shaping is off by default in Virtuozzo and is controlled by the `TRAFFIC_SHAPING` parameter in the `/etc/vz/vz.conf` global configuration file.

[NOTE]
[subs="quotes"]
====
*Note:* Incoming traffic cannot be controlled for containers in Virtuozzo.
====

To turn traffic shaping on, you need to complete the following steps:

[options="compact"]
. Set the value of `TRAFFIC_SHAPING` to `yes` in the global configuration file.
. Correctly set up the `BANDWIDTH` and `TOTALRATE` parameters values.
. Start traffic shaping with the `/etc/init.d/vz shaperon` command.

The `BANDWIDTH` variable is used for specifying the network rate, in kilobits per second, of available network adapters. By default, it is set to `enp0s5:100000`, which corresponds to a 100Mb/s Fast Ethernet card. If your Hardware Node has more network adapters installed, update this parameter by listing all the adapters participating in shaping. For example, if you have two Fast Ethernet cards, set the parameter to `enp0s5:100000 enp0s6:100000`.

The `TOTALRATE` variable specifies the size of the so-called bandwidth pool for each network class being shaped. The bandwidth from the pool can be borrowed by containers when they need more bandwidth for communicating with hosts from the corresponding network class. It is used to limit the total available outgoing traffic containers can consume. The format of this variable is +__<NIC>__:__<network_class>__:__<bandwidth_in_Kbits_per_second>__+ and defines the pool size per network class for a given network adapter. Multiple entries for different network classes and adapters can be separated by spaces. The default value for `TOTALRATE` is `enp0s5:1:4000`, which corresponds to the pool size of 4Mb/s for Network Class 1 on the first Ethernet adapter.

In the `/etc/vz/vz.conf` configuration file, you can also define the `RATE` variable whose value amounts to the number of kilobits per second any container is guaranteed to receive for outgoing traffic with a network class on an Ethernet device. The default value of this parameter is `enp0s5:1:8`, which means that any container is guaranteed to receive the bandwidth of at least 8 Kbps for sending data to Class 1 hosts on the first Ethernet device. This bandwidth is not the limit for a container (unless the `RATEBOUND` parameter is enabled for the container); the container can take the needed bandwidth from the `TOTALRATE` bandwidth pool if it is not used by other containers.

After setting up the above parameters, start bandwidth management as follows:

[subs="quotes"]
----
*# /etc/init.d/vz shaperon*
Starting shaping: Ok
Set shaping on running container :
vz WARNING: Can't get tc class for container(MyCT1).
vz WARNING: Can't access file /var/run/vz_tc_classes. \
Creating new one.
vz WARNING: Can't get tc class for container(1).
----

Now you have activated the network bandwidth limits. To turn traffic shaping off temporarily, use the `/etc/init.d/vz shaperoff` command. If you want to disable bandwidth management permanently, set the `TRAFFIC_SHAPING` variable to `no` in the `/etc/vz/vz.conf` configuration file.

Configuring Network Bandwidth Management
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The network bandwidth for outgoing traffic a container receives is controlled by two parameters: `RATE` and `RATEBOUND`.

[NOTE]
[subs="quotes"]
====
*Note:* Incoming traffic cannot be controlled in the current version of Virtuozzo.
====

The `RATE` parameter specifies the guaranteed outgoing traffic rate that a container receives. This rate can be specified differently for different network classes. Bandwidth values are specified in kilobits per second (Kbps). It is recommended to increase this value in 8 Kbps increments and set it to at least 8 Kbps. The example below demonstrates how to set the `RATE` parameter for the container MyCT1 to 16 Kbps for network class 1 on the `enp0s5` network adapter:

[subs="quotes"]
----
*# prlctl set MyCT1 --rate enp0s5:1:16*
----

[NOTE]
[subs="quotes"]
====
*Note:* For containers, you can also configure the `RATE` parameter for different network adapters. The rates for all other network adapters can be configured in the `/etc/vz/vz.conf` global configuration file.
====

The `RATEBOUND` parameter specifies whether the network bandwidth available to container for outgoing traffic is limited by the bandwidth specified in the `RATE` variable. By default, this parameter is turned off for all newly created containers. That means that containers are allowed to take free bandwidth from the `TOTALRATE` pool. You can turn the `RATEBOUND` parameter on by using the `--ratebound` option of the `prlctl set` command, for example:

[subs="quotes"]
----
*# prlctl set MyCT1 --ratebound on*
----

The actual network bandwidth available to containers depends on the number of containers and the total sum of the `RATE` values, and normally does not coincide with the bandwidth specified in their own `RATE` parameters. If the `RATEBOUND` parameter is turned on, the container bandwidth is limited by the value of the `RATE` parameter.

If the the `RATE` and `RATEBOUND` parameters are not set for individual containers, the values from the `/etc/vz/vz.conf` configuration file are taken. By default, Virtuozzo does not set `RATEBOUND`, which corresponds to `no`, and `RATE` is set to `enp0s5:1:8`.

The network bandwidth management in Virtuozzo works in the following way. The bandwidth pool for a given network class (configurable through the `TOTALRATE` variable in the global configuration file) is divided among the containers transmitting data proportionally to their `RATE` settings. If the total value of the `RATE` variables of all containers transmitting data does not exceed the `TOTALRATE` value, each container gets the bandwidth equal or greater than its `RATE` value (unless the `RATEBOUND` variable is enabled for this container). If the total value of the `RATE` variables of all containers transmitting data exceeds the `TOTALRATE` value, each container may get less than its `RATE` value.

Once you configure the bandwidth settings, activate your changes by running the following command:

[subs="quotes"]
----
*# /etc/init.d/vz shaperrestart*
Stopping shaping: Ok
Starting shaping: Ok
Set shaping on running container: Ok
----

This command clears off all existing shaping settings and sets them again using the configuration files of running containers.

Managing Disk I/O Parameters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This section explains how to manage disk input and output (I/O) parameters in Virtuozzo systems.

Configuring Priority Levels for Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In Virtuozzo, you can configure the disk I/O (input/output) priority level of containers. The higher the I/O priority level, the more time the container will get for its disk I/O activities as compared to the other containers on the Hardware Node. By default, any container on the Hardware Node has the I/O priority level set to 4. However, you can change the current I/O priority level in the range from 0 to 7 using the `--ioprio` option of the `prlctl set` command. For example, you can issue the following command to set the I/O priority of container MyCT1 to 6:

[subs="quotes"]
----
*# prlctl set MyCT1 --ioprio 6*
----

To check the I/O priority level currently applied to container MyCT1, you can execute the following command:

[subs="quotes"]
----
*# grep IOPRIO /etc/vz/conf/26bc47f6-353f-444b-bc35-b634a88dbbcc.conf*
IOPRIO="6"
----

Configuring Disk I/O Bandwidth
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In Virtuozzo, you can configure the bandwidth containers are allowed to use for their disk input and output (I/O) operations. Limiting the disk I/O bandwidth can help you prevent the situations when high disk activities in one container (generated, for example, by transferring huge amounts of data to/from the container) can slow down the performance of other containers on the hardware node.

By default, the I/O bandwidth limit for all newly created containers is set to 0, which means that no limits are applied to any containers. To limit the disk I/O bandwidth for a container, you can use the `--iolimit` option of the `prlctl set` command. For example, the following command sets the I/O bandwidth limit for the container MyCT1 to 10 megabytes per second (MB/s):

[subs="quotes"]
----
*# prlctl set MyCT1 --iolimit 10*
Set up iolimit: 10485760
Saved parameters for container MyCT1
----

By default, the limit is set in megabytes per second. However, you can use the following suffixes to use other measurement units:

[options="compact"]
* `G` sets the limit in gigabytes per second (1G).
* `K` sets the limit in kilobytes per second (10K).
* `B` sets the limit in bytes per second (10B).

[NOTE]
[subs="quotes"]
====
*Note:* In the current version of Virtuozzo, the maximum I/O bandwidth limit you can set for a container is 2 GB per second.
====

To check that the I/O speed limit has been successfully applied to the container MyCT1, use the `prlctl list` command:

[subs="quotes"]
----
*# prlctl list MyCT1 -o iolimit*
IOLIMIT
10485760
----

At any time, you can remove the I/O bandwidth limit set for container MyCT1 by running this command:

[subs="quotes"]
----
*# prlctl set MyCT1 --iolimit 0*
Set up iolimit: 0
Saved parameters for container MyCT1
----

Configuring the Number of I/O Operations Per Second
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In Virtuozzo, you can limit the maximum number of disk input and output operations per second containers are allowed to perform (known as the IOPS limit). You may consider setting the IOPS limit for containers with high disk activities to ensure that they do not affect the performance of other containers on the Node.

[NOTE]
[subs="quotes"]
====
*Note:* By default all I/O inside containers is cached and the direct access flag (`O_DIRECT`) is ignored when opening files. This significantly reduces the number of IOPS required for container workload and helps avoid I/O bottlenecks on the Node. For instructions on how to configure honoring of the `O_DIRECT` flag inside containers, see <<_setting_the_direct_access_flag_inside_containers>> below.
====

By default, IOPS is not limited for newly created containers. To set the IOPS limit, you can use the `--iopslimit` option of the `prlctl set` command. For example, to allow container MyCT1 to perform no more than 100 disk I/O operations per second, you can run the following command:

[subs="quotes"]
----
*# prlctl set MyCT1 --iopslimit 100*
----

To ensure that the IOPS limit has been successfully applied to container MyCT1, use the `pstat -A` command:

[subs="quotes"]
----
*# pstat -A*
ST IOUSED IOWAIT IO          IOPS      NAME
OK   0.00  0.00  0.0/---KB/s 0.0/100/s MyCT1
----

The *IOPS* column shows the IOPS limits currently applied to container MyCT1.

At any time, you can remove the set IOPS limits by running this command:

[subs="quotes"]
----
*# prlctl set MyCT1 --iopslimit 0*
----

Setting the Direct Access Flag Inside Containers
++++++++++++++++++++++++++++++++++++++++++++++++

You can configure honoring of the `O_DIRECT` flag inside containers with the `sysctl` parameter `fs.odirect_enable`:

[options="compact"]
* To ignore the `O_DIRECT` flag inside a container, set `fs.odirect_enable` to `0` in that container.
* To honor the `O_DIRECT` flag inside the container, set `fs.odirect_enable` to `1` in that container.
* To have a container inherit the setting from the Hardware Node, set `fs.odirect_enable` to `2` in that container (default value). On the Hardware Node, `fs.odirect_enable` is `0` by default.

[NOTE]
[subs="quotes"]
====
*Note:* The `fs.odirect_enable` parameter on the Node only affects honoring of the `O_DIRECT` flag in containers and not on the Node itself where the `O_DIRECT` flag is always honored.
====

Viewing Disk I/O Statistics
^^^^^^^^^^^^^^^^^^^^^^^^^^^

In Virtuozzo 7 beta, you can view disk input and output (I/O) statistics for containers. To display the I/O statistics for all running containers on the physical server, you can run this command:

[subs="quotes"]
----
*# vzstat -a*
----

The information related to the containers disk I/O statistics is at the end of the command output. The table below explains the displayed I/O parameters:

[options="header",cols="1,3"]
|====
|Parameter|Description

|`IOUSED%`|Percentage of time the disks are used by the container.
|`IOWAIT%`|Percentage of time when at least one I/O transaction in the container is waiting for being served.
|`IO`|I/O rate and limit, in bytes, kilobytes, megabytes, or gigabytes per second.
|`IOPS`|I/O operations rate and limit, in operations per second.
|====

Managing Memory Parameters for Containers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This section describes the VSwap memory management system introduced in Virtuozzo 7 beta. You will learn to do the following:

[options="compact"]
* Configure the main VSwap parameters for containers.
* Set the memory allocation limit in containers.
* Configure OOM killer behavior.
* Enhance the VSwap functionality.

Configuring Main VSwap Parameters
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Virtuozzo utilizes the VSwap scheme for managing memory-related parameters in containers. Like many other memory management schemes used on standalone Linux computers, this scheme is based on two main parameters:

[options="compact"]
* `RAM` determines the total size of RAM that can be used by the processes of a container.
* `swap` determines the total size of swap that can be used by a container for swapping out memory once the RAM is exceeded.

[NOTE]
[subs="quotes"]
====
*Note:* You can also set memory limits for and provide memory guarantees to containers by configuring multiple UBC (User Beancounter) parameters (`numproc`, `numtcpsock`, `vmguarpages`, and so on). These parameters provide you with comprehensive facilities of customizing the memory resources in respect of your containers. However, this way of managing system resources is more complex and requires more effort to be made on your part to adopt it to your system.
====

The memory management scheme works as follows:

[options="compact"]
. You set for a container a certain amount of RAM and swap space that can be used by the processes running in the container.
. When the container exceeds the RAM limit set for it, the swapping process starts. The swapping process for containers slightly differs from that on a standalone computer. The container swap file is virtual and, if possible, resides in the Node RAM. In other words, when the swap-out for a container starts and the Node has enough RAM to keep the swap file, the swap file is stored in the Node RAM rather than on the hard drive.
. Once the container exceeds its swap limit, the system invokes the OOM Killer for this container.
. The OOM Killer chooses one or more processes running in the affected container and forcibly kills them.

By default, any newly created container starts using the new memory management scheme. To find out the amount of RAM and swap space set for a container, you can check the values of the `PHYSPAGES` and `SWAPPAGES` parameters in the container configuration file, for example:

[subs="quotes"]
----
*# grep PHYSPAGES /etc/vz/conf/26bc47f6-353f-444b-bc35-b634a88dbbcc.conf*
PHYSPAGES="65536:65536"
*# grep SWAPPAGES /etc/vz/conf/26bc47f6-353f-444b-bc35-b634a88dbbcc.conf*
SWAPPAGES="65536"
----

In this example, the value of the `PHYSPAGES` parameter for container MyCT1 is set to 65536. The `PHYSPAGES` parameter displays the amount of RAM in 4-KB pages, so the total amount of RAM set for container MyCT1 equals to 256 MB. The value of the `SWAPPAGES` parameter is also set to 256 MB.

To configure the amounts of RAM and swap space for container MyCT1, use the `--memsize` and `--swappages` options of the `prlctl set` command. For example, you can execute the following command to set the amount of RAM and SWAP in container MyCT1 to 1 GB and 512 MB, respectively:

[subs="quotes"]
----
*# prlctl set MyCT1 --memsize 1G --swappages 512M*
----

Configuring the Memory Allocation Limit
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When an application starts in a container, it allocates a certain amount of memory for its needs. Usually, the allocated memory is much more than the application actually requires for its execution. This may lead to a situation when you cannot run an application in the container even if it has enough free memory. To deal with such situations, the VSwap memory management scheme introduces the new `vm_overcommit` option. Using it, you can configure the amount of memory applications in a container may allocate, irrespective of the amount of RAM and swap space assigned to the container.

The amount of memory that can be allocated by applications of a container is the sum of RAM and swap space set for this container multiplied by a memory overcommit factor. In the default (basic) container configuration file, this factor is set to 1.5. For example, if a container is based on the default configuration file and assigned 1 GB of RAM and 512 MB of swap, the memory allocation limit for the container will be 2304 MB. You can configure this limit and set it, for example, to 3 GB by running this command:

[subs="quotes"]
----
*# vzctl set MyCT1 --vm_overcommit 2 --save*
----

This command uses the factor of 2 to increase the memory allocation limit to 3 GB: +
(1 GB of RAM + 512 MB of swap) * 2 = 3 GB

Now applications in container MyCT1 can allocate up to 3 GB of memory, if necessary.

Configuring OOM Killer Behavior
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The OOM killer selects container process (or processes) to end based on the badness reflected in +/proc/_&lt;pid&gt;_/oom_score+. The badness is calculated using process memory, total memory, and badness adjustment, and then clipped to the range from 0 to 1000. Each badness point stands for one thousandth of container memory. The process to be killed is the one with the highest resulting badness.

The OOM killer for container processes can be configured using the `/etc/vz/oom-groups.conf` file that lists patterns based on which badness adjustment is selected for each running process. Each pattern takes a single line and includes the following columns:

[options="compact"]
* +_&lt;command&gt;_+, mask for the task command name;
* +_&lt;parent&gt;_+, mask for the parent task name;
* +_&lt;oom_uid&gt;_+, task user identifier (UID) filter:
+
--
[options="compact"]
* If +_&lt;oom_uid&gt;_+ is -1, the pattern will be applicable to tasks with any UIDs,
* If +_&lt;oom_uid&gt;_+ is 0 or higher, the pattern will be applicable to tasks with UIDs equal to the +_&lt;oom_uid&gt;_+ value,
* If +_&lt;oom_uid&gt;_+ is less than -1, the pattern will be applicable to tasks with UIDs less than the negative +_&lt;oom_uid&gt;_+ value);
--
* +_&lt;oom_score_adj&gt;_+ badness adjustment. As with badness itself, each adjustment point stands for one thousandth of total container memory. Negative adjustment values reduce process badness. In an out-of-memory situation, an adjustment will guarantee that the process will be allowed to occupy at least +_&lt;oom_score_adj&gt;_+ thousandths of container memory while there are other processes with higher badness running in the container.

[NOTE]
[subs="quotes"]
====
*Note:* The +_&lt;command&gt;_+ and +_&lt;parent&gt;_+ masks support wildcard suffixes: asterisk (*) matches any suffix. E.g., "foo" matches only "foo", "foo*" matches "foo" and "foobar".
====

For example, the pattern

----
sshd     init     -500     -100
----

means that in an out-of-memory situation, `sshd`, a child of `init`, will be guaranteed at least 100 thousandths (i.e., 10%) of container memory, if its UID is less than -(-500) or just 500, e.g., 499. According to RHEL conventions, UIDs from 1 to 499 are usually reserved for system use, so such delimitation may be useful to prioritize and save system processes.

While calculating the badness of a process, the OOM killer searches `/proc/vz/oom_score_adj` for a suitable pattern based on masks and task UID filter. The search starts from the first line and ends when the first suitable pattern is found. The corresponding adjustment value is then used to obtain the resulting process badness.

The data from `/etc/vz/oom-groups.conf` is reset and committed to the kernel on boot. To reset and commit the config file manually, you can use the following command:

[subs="quotes"]
----
*# cat /etc/vz/oom-groups.conf &gt; /proc/vz/oom_score_adj*
----

Tuning VSwap
^^^^^^^^^^^^

The VSwap management scheme can be extended by using UBC parameters. For example, you can set the `numproc` parameter to configure the maximal number of processes and threads a container may create or the `numfile` parameter to specify the number of files that may be opened by all processes in the container.

Managing Container Resources Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Any container is configured by means of its own configuration file. You can manage container configurations in a number of ways:

. Using configuration sample files shipped with Virtuozzo. These files are used when a new container is being created (for details, see <<_virtuozzo_containers>>). Currently, the following configuration sample files are provided:
+
--
[options="compact"]
* `basic` for creating standard containers.
* `confixx` for creating containers that are to run the Confixx control panel.
* `vswap.plesk` for creating containers with the Plesk control panel.
* `vswap.256MB` for creating containers with 256 MB of main memory.
* `vswap.512Mb` for creating containers with 512 MB of main memory.
* `vswap.1024Mb` for creating containers with 1024 MB of main memory.
* `vswap.2048Mb` for creating containers with 2048 MB of main memory.
--
+
[NOTE]
[subs="quotes"]
====
*Note:* Configuration sample files cannot contain spaces in their names.
====
+
Any sample configuration file can also be applied to an existing container. You would do this if, for example, you want to upgrade or downgrade the overall resources configuration of a particular container:
+
[subs="quotes"]
----
*# prlctl set MyCT1 --applyconfig basic*
----
+
This command applies all the parameters from the `ve-basic.conf-sample` file to container MyCT1. When you install Virtuozzo on your Hardware Node, the default container samples are put to the `/etc/vz/conf` directory. They have the following format: +ve-_&lt;name&gt;_.conf-sample+ (for example, `ve-basic.conf-sample`).

. Using specific utilities for preparing configuration files in their entirety. The tasks these utilities perform are described in the following subsections of this section.
. The direct creating and editing of the corresponding container configuration file (+/etc/vz/conf/_&lt;UUID&gt;_.conf+). This can be performed with the help of any text editor. The instructions on how to edit container configuration files directly are provided in the four preceding sections. In this case you have to edit all the configuration parameters separately, one by one.

Splitting Server Into Equal Pieces
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

It is possible to create a container configuration roughly representing a given fraction of the Hardware Node. If you want to create such a configuration that up to 20 fully loaded containers would be able to be simultaneously running on the given Hardware Node, you can do it as follows:

[subs="quotes"]
----
*# cd /etc/vz/conf*
*# vzsplit -n 20 -f mytest*
Config /etc/vz/conf/ve-mytest.conf-sample was created
----

Notice that the configuration produced depends on the given Hardware Node resources. Therefore, it is important to validate the resulted configuration file before trying to use it, which is done with the help of the `vzcfgvalidate` utility. For example:

[subs="quotes"]
----
*# vzcfgvalidate ve-mytest.conf-sample*
Validation completed: success
----

The command output shows that the configuration file is valid and you can safely use as the basis for creating new containers.

[NOTE]
[subs="quotes"]
====
*Note:* If you generate a container configuration sample using the `vzsplit` command line utility, the resulting container sample is put to the `/etc/vz/conf` directory. This sample can then be used by `prlctl create` when creating a new container on its basis.
====

Scaling Container Configuration
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Any configuration or configuration sample file can prove insufficient for your needs. You might have an application which does not fit into existing configurations. The easiest way of producing a container configuration is to scale an existing one.

Scaling produces a "heavier" or "lighter" configuration in comparison with an existing one. All the parameters of the existing configuration are multiplied by a given number. A heavier configuration is produced with a factor greater than 1, and a lighter one--with a factor between 0 and 1.

[NOTE]
[subs="quotes"]
====
*Note:* If you create a new sample on the basis of an existing sample using the `vzcfgscale` command line utility, the resulting container sample is put to the `/etc/vz/conf` directory. This sample can then be used by `prlctl create` when creating a new container on its basis.
====

The session below shows how to produce a configuration sample 50% heavier than the `basic` configuration shipped with Virtuozzo:

[subs="quotes"]
----
*# cd /etc/vz/conf*
*# vzcfgscale -a 1.5 -o ve-improved.conf-sample ve-basic.conf-sample*
*# vzcfgvalidate ve-improved.conf-sample*
Validation completed: success
----

Now you can use the `improved` configuration file to create new containers.

It is possible to use the same technique for scaling configurations of the existing containers. Notice that the output file cannot be the same as the file being scaled. You have to save the scaling results into an intermediate file.

Applying New Configuration Samples to Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Virtuozzo allows you to change the configuration sample file a container is based on and, thus, to modify all the resources the container may consume and/or allocate at once. For example, if container MyCT1 is currently based on the `basic` configuration sample and you are planning to run the Plesk application inside the container, you may wish to apply the `vswap.plesk` sample to it instead of `basic`, which will automatically adjust the necessary container resource parameters for running the Plesk application inside container MyCT1. To do this, you can execute the following command on the Hardware Node:

[subs="quotes"]
----
*# prlctl set MyCT1 --applyconfig vswap.plesk*
----

This command reads the resource parameters from the `ve-vswap.plesk.conf-sample` file located in the `/etc/vz/conf` directory and applies them one by one to container MyCT1.

When applying new configuration samples to containers, keep in mind the following:

[options="compact"]
* All container sample files are located in the `/etc/vz/conf` directory on the Hardware Node and are named according to the following pattern: +ve-_&lt;name&gt;_.conf-sample+. You should specify only the +_&lt;name&gt;_+ part of the corresponding sample name after the `--applyconfig` option (`vswap.plesk in the example above).
* The `--applyconfig` option applies all the parameters from the specified sample file to the given container, except for the `OSTEMPLATE`, `TEMPLATES`, `VE_ROOT`, `VE_PRIVATE`, `HOSTNAME`, `IP_ADDRESS`, `TEMPLATE`, `NETIF` parameters (if they exist in the sample file).

You may need to restart your container depending on the fact whether the changes for the selected parameters can be set on the fly or not. If some parameters could not be configured on the fly, you will be presented with the corresponding message informing you of this fact.

Monitoring Resources
~~~~~~~~~~~~~~~~~~~~

In Virtuozzo, you can use the `vzstat` utility to monitor system resources in real time. When executed, the utility displays the status and load of the system: its disk, network, CPU, memory, and other parameters. It also provides the list of running containers together with their resources consumption statistics. For example, you can run the following command on the server to view your current system resources:

[subs="quotes"]
----
*# vzstat -d 5*
 12:47pm, up  1:32,  2 users, load average: 0.00, 0.04, 0.05
CTNum 1, procs  240: R   1, S  239, D   0, Z   0, T   0, X   0
CPU [ OK ]: CTs   0%, CT0   0%, user   0%, sys   0%, idle 100%, lat(ms)   5/0
Mem [ OK ]: total 985MB, free 217MB, lat(ms) 0/0
  ZONE0 (DMA): size 15MB, act 3MB, inact 5MB, free 5MB (0/0/1)
  ZONE1 (DMA32): size 1007MB, act 220MB, inact 437MB, free 212MB (43/54/64)
  Mem lat (ms): A0 0, K0 0, U0 0, K1 0, U1 0
  Slab pages: 68MB/68MB (ino 30MB, de 0MB, bh 5MB, pb 0MB)
Swap [ OK ]: tot 2047MB, free 2047MB, in 0.000MB/s, out 0.000MB/s
Net [ OK ]: tot: in  0.002MB/s   30pkt/s, out  0.002MB/s    2pkt/s
            br0: in  0.001MB/s   15pkt/s, out  0.001MB/s    1pkt/s
             lo: in  0.000MB/s    0pkt/s, out  0.000MB/s    0pkt/s
     virbr1-nic: in  0.000MB/s    0pkt/s, out  0.000MB/s    0pkt/s
         enp0s5: in  0.001MB/s   15pkt/s, out  0.001MB/s    1pkt/s
         virbr1: in  0.000MB/s    0pkt/s, out  0.000MB/s    0pkt/s
Disks [ OK ]: in 0.000MB/s, out 0.000MB/s

    CTID ST   %VM    %KM        PROC     CPU     SOCK FCNT MLAT IP
26bc47f6 OK 4.4/52  0.5/MAX   0/19/MAX 0.00/100   0/MAX    0    0
----

The command output is updated with the time interval equal to the value specified after the `-d` (delay) option measured in seconds. In the session above, the statistics displayed is renewed every five seconds. If the `-d` option is not specified, the default interval equals 1 second.

As you can see, the utility provides real-time information on all main resources subsystems pertaining both to the physical server and to its containers: the disk, network, CPU, and memory subsystems. You may want to shrink the output of the utility by specifying the `-b` (brief) option instead of `-v` (verbose), or to do without any options to use the "normal" mode of displaying.

The following information is displayed by default per each container:

[options="header",cols="1,4"]
|====
|Column|Description

|ST|Container status. If there are no failed counters and the latency values are normal, the status is "OK". Otherwise, it is displayed in red as "!!". You can sort containers by their status to see the problem containers first.
|%VM|Virtual memory usage, in per cent to the total memory. The first number is how much virtual memory is being used, and the second one is the virtual memory barrier.
|%KM|Kernel memory usage, in per cent to the normal zone size. The first number is how much kernel memory is being used, and the second one is the kernel memory barrier.
|CPU|CPU usage in per cent to all available CPUs. The first number is how much of the CPU power is being used by the container, and the second one is its guaranteed share judging by the `cpuunits` parameter. Note that the actual CPU usage may be higher than the guaranteed one.
|FCNT|The number of failed counters for all the resource parameters. In the standard mode of displaying, this number represents the increase of failed counters since the previous screen update, whereas in the average mode of displaying, it represents an absolute failed counters sum for the given container.
|MLAT|Maximal scheduling latency for the container, in ms. This parameter shows the maximal scheduling latency inside the given container, i.e. for how long (at the utmost) a process inside the container awaits for the CPU.
|NAME|Container name.
|====

The *%VM*, *%KM*, and *CPU* columns provide two values per column separated by a slash for each container. The first value indicates the real usage of the corresponding parameter by the container, and the second one--the maximal value allowed for the container.

Managing Services and Processes
-------------------------------

This chapter provides information on what services and processes are, how they influence the operation and performance of your system, and what tasks they perform in the system.

You will learn how to use the command line utilities in order to manage services and processes in Virtuozzo. In particular, you will learn how to monitor active processes in your system, change the mode of the `xinetd`-dependent services, identify the container UUID where a process is running by the process ID, start, stop, or restart services and processes, and edit the service run levels.

What Are Services and Processes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Instances of any programs currently running in the system are referred to as processes. A process can be regarded as the virtual address space and the control information necessary for the execution of a program. A typical example of a process is the _vi_ application running on your server or inside your Linux-based containers. Along with common processes, there are a great number of processes that provide an interface for other processes to call. They are called services. In many cases, services act as the brains behind many crucial system processes. They typically spend most of their time waiting for an event to occur or for a period when they are scheduled to perform some task. Many services provide the possibility for other servers on the network to connect to the given one via various network protocols. For example, the `nfs` service provides the NFS server functionality allowing file sharing in TCP/IP networks.

You may also come across the term "daemon" that is widely used in connection with processes and services. This term refers to a software program used for performing a specific function on the server system and is usually used as a synonym for "service". It can be easily identified by `d` at the end of its name. For example, `httpd` (HTTP daemon) represents a program that runs in the background of your system and waits for incoming requests to a web server. The daemon answers the requests automatically and serves the hypertext and multimedia documents over the Internet using HTTP.

When working with services, you should keep in mind the following. During the lifetime of a service, it uses many system resources. It uses the CPUs in the system to run its instructions and the system's physical memory to hold itself and its data. It opens and uses files within the file systems and may directly or indirectly use certain physical devices in the system. Therefore, in order not to decrease your system performance, you should run only those services on the hardware node that are really needed at the moment.

Besides, you should always remember that running services in the Host OS is much more dangerous than running them in containers. In case violators get access to one of the containers through any running service, they will be able to damage only the container where this service is running, but not the other containers on your server. The hardware node itself will also remain unhurt. And if the service were running on the hardware node, it would damage both the server and all containers residing on it. Thus, you should make sure that you run only those services on the server that are really necessary for its proper functioning. Launch all additional services you need at the moment inside separate containers. It can significantly improve your system safety.

Main Operations on Services and Processes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ability to monitor and control processes and services in your system is essential because of the profound influence they have on the operation and performance of your whole system. The more you know about what each process or service is up to, the easier it will be to pinpoint and solve problems when they creep in.

The most common tasks associated with managing services running on the hardware node or inside a container are starting, stopping, enabling, and disabling a service. For example, you might need to start a service in order to use certain server-based applications, or you might need to stop or pause a service in order to perform testing or to troubleshoot a problem.

For `xinetd`-dependent services, you do not start and stop but enable and disable services. The services enabled in this way are started and stopped on the basis of the corresponding state of the `xinetd` daemon. Disabled services are not started whatever the `xinetd` state.

In Virtuozzo, you can manage services on the hardware node and inside containers by means of special Linux command-line utilities. You can do it either locally or from any server connected on the network.

As for processes, such Virtuozzo utilities as `vzps`, `vztop`, `vzpid` enable you to see what a process is doing and to control it. Sometimes, your system may experience problems such as slowness or instability, and using these utilities can help you improve your ability to track down the causes. It goes without saying that in Virtuozzo you can perform all those operations on processes you can do in a normal system, for example, kill a process by sending a terminate signal to it.

Managing Processes and Services
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In Virtuozzo, services and processes can be managed using the following command-line utilities:

[options="compact"]
* `vzps`
* `vzpid`
* `vztop`

With their help, you can perform the following tasks:

[options="compact"]
* print the information about active processes on your hardware node
* view the processes activity in real time
* change the mode of the services that can be either `xinetd`-dependent or standalone
* identify the container UUID where a process is running by the process ID

Viewing Active Processes and Services
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The `vzps` utility provides certain additional functionality related to monitoring separate containers running on the hardware node. For example, you can use the `-E` switch with the `vzps` utility to:

[options="compact"]
* display the container UUIDs where the processes are running
* view the processes running inside a particular container

`vzps` prints the information about active processes on your hardware node. When run without any options, `vzps` lists only those processes that are running on the current terminal. Below is an example output of `vzps`:

[subs="quotes"]
----
*# vzps*
  PID TTY         TIME CMD
 4684 pts/1   00:00:00 bash
27107 pts/1   00:00:00 vzps
----

Currently, the only processes assigned to the user/terminal are the `bash` shell and the `vzps` command itself. In the output, the PID (Process ID), TTY, TIME, and CMD fields are contained. TTY denotes which terminal the process is running on, TIME shows how much CPU time the process has used, and CMD is the name of the command that started the process.

[NOTE]
[subs="quotes"]
====
*Note:* The IDs of the processes running inside containers and displayed by running the `vzps` command on the hardware node does not coincide with the IDs of the same processes shown by running the `ps` command inside these containers.
====

As you can see, the standard `vzps` command just lists the basics. To get more details about the processes running on your server, you will need to pass some command line arguments to `vzps`. For example, using the `aux` arguments with this command displays processes started by other users (`a`), processes with no terminal or one different from yours (`x`), the user who started the process and when it began (`u`).

[subs="quotes"]
----
*# vzps aux*
USER  PID %CPU %MEM   VSZ  RSS TTY   STAT START  TIME COMMAND
root    1  0.0  0.0  1516  128 ?     S   Jul14   0:37 init
root    5  0.0  0.0     0    0 ?     S   Jul14   0:03 [ubstatd]
root    6  0.0  0.0     0    0 ?     S   Jul14   3:20 [kswapd]
#27     7  0.0  0.0     0    0 ?     S   Jul14   0:00 [bdflush]
root    9  0.0  0.0     0    0 ?     S   Jul14   0:00 [kinoded]
root 1574  0.0  0.1   218  140 pts/4 S   09:30   0:00 -bash
----

There is a lot more information now. The fields *USER*, *%CPU*, *%MEM*, *VSZ*, *RSS*, *STAT*, and *START* have been added. Let us take a quick look at what they tell us.

The *USER* field shows you which user initiated the command. Many processes begin at system start time and often list root or some system account as the user. Other processes are, of course, run by actual users.

The *%CPU*, *%MEM*, *VSZ*, and *RSS* fields all deal with system resources. First, you can see what percentage of the CPU the process is currently utilizing. Along with CPU utilization, you can see the current memory utilization and its VSZ (virtual memory size) and RSS (resident set size). VSZ is the amount of memory the program would take up if it were all in memory. RSS is the actual amount currently in memory. Knowing how much a process is currently eating will help determine if it is acting normally or has spun out of control.

You will notice a question mark in most of the TTY fields in the `vzps aux` output. This is because most of these programs were started at boot time and/or by initialization scripts. The controlling terminal does not exist for these processes; thus, the question mark. On the other hand, the `bash` command has a TTY value of pts/4. This is a command being run from a remote connection and has a terminal associated with it. This information is helpful for you when you have more than one connection open to the machine and want to determine which window a command is running in.

STAT shows the current status of a process. In our example, many are sleeping, indicated by an S in the STAT field. This simply means that they are waiting for something. It could be user input or the availability of system resources. The other most common status is R, meaning that it is currently running.

You can also use the `vzps` command to view the processes inside any running container. The example below shows you how to display all active processes inside container MyCT1 with UUID 26bc47f6-353f-444b-bc35-b634a88dbbcc:

[subs="quotes"]
----
*# vzps -E 26bc47f6-353f-444b-bc35-b634a88dbbcc*
                                CTID     PID TTY          TIME CMD
26bc47f6-353f-444b-bc35-b634a88dbbcc   14663 ?        00:00:00 init
26bc47f6-353f-444b-bc35-b634a88dbbcc   14675 ?        00:00:00 kthreadd/26bc47
26bc47f6-353f-444b-bc35-b634a88dbbcc   14676 ?        00:00:00 khelper
26bc47f6-353f-444b-bc35-b634a88dbbcc   14797 ?        00:00:00 udevd
26bc47f6-353f-444b-bc35-b634a88dbbcc   15048 ?        00:00:00 rsyslogd
26bc47f6-353f-444b-bc35-b634a88dbbcc   15080 ?        00:00:00 sshd
26bc47f6-353f-444b-bc35-b634a88dbbcc   15088 ?        00:00:00 xinetd
26bc47f6-353f-444b-bc35-b634a88dbbcc   15097 ?        00:00:00 saslauthd
26bc47f6-353f-444b-bc35-b634a88dbbcc   15098 ?        00:00:00 saslauthd
26bc47f6-353f-444b-bc35-b634a88dbbcc   15116 ?        00:00:00 sendmail
26bc47f6-353f-444b-bc35-b634a88dbbcc   15125 ?        00:00:00 sendmail
26bc47f6-353f-444b-bc35-b634a88dbbcc   15134 ?        00:00:00 httpd
26bc47f6-353f-444b-bc35-b634a88dbbcc   15139 ?        00:00:00 httpd
26bc47f6-353f-444b-bc35-b634a88dbbcc   15144 ?        00:00:00 crond
26bc47f6-353f-444b-bc35-b634a88dbbcc   15151 ?        00:00:00 mingetty
26bc47f6-353f-444b-bc35-b634a88dbbcc   15152 ?        00:00:00 mingetty
----

Monitoring Processes in Real Time
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The `vztop` utility is rather similar to `vzps` but is usually started full-screen and updates continuously with process information. This can help with programs that may infrequently cause problems and can be hard to see with `vzps`. Overall system information is also presented, which makes a nice place to start looking for problems.

The `vztop` utility can be run on the server just as the standard Linux `top` utility. The only features that distinguish the `vztop` utility from `top` are the following:

[options="compact"]
* `vztop` allows you to use the `-E` option that monitors only the processes belonging to the container whose processes you want to display.
* You can press `e` to temporarily view/hide the UUIDs where the processes are running.
* You can press `E` to set the filter on the UUID field that helps you display only the processes belonging to the given container.

The `vztop` utility usually has an output like the following:

[subs="quotes"]
----
*# vztop -E 26bc47f6-353f-444b-bc35-b634a88dbbcc*
vztop - 12:54:13 up  1:38,  2 users,  load average: 0.00, 0.01, 0.05
Tasks:  16 total,   0 running,  16 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.0 us,  0.2 sy,  0.0 ni, 99.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  1009128 total,   235884 free,   129812 used,   643432 buff/cache
KiB Swap:  2097148 total,  2097088 free,       60 used.   712756 avail Mem

        CTID   PID USER  PR  NI  VIRT  RES  SHR S  %CPU %MEM   TIME+ COMMAND
26bc47f6-... 14663 root  20 0 19284 1456 1208 S 0.0  0.1 0:00.01 init
26bc47f6-... 14675 root  20 0   0  0  0 S 0.0  0.0 0:00.00 kthreadd/26bc47
26bc47f6-... 14676 root  20 0   0  0  0 S 0.0  0.0 0:00.00 khelper
26bc47f6-... 14797 root  16  -4 10692  700  400 S 0.0  0.1 0:00.00 udevd
26bc47f6-... 15048 root  20 0  177452 1336  984 S 0.0  0.1 0:00.00 rsyslogd
26bc47f6-... 15080 root  20 0 66268 1124  416 S 0.0  0.1 0:00.00 sshd
26bc47f6-... 15088 root  20 0 21768  904  672 S 0.0  0.1 0:00.00 xinetd
26bc47f6-... 15097 root  20 0 66456  896  252 S 0.0  0.1 0:00.00 saslauthd
26bc47f6-... 15098 root  20 0 66456  648  4 S 0.0  0.1 0:00.00 saslauthd
26bc47f6-... 15116 root  20 0 82632 2388  700 S 0.0  0.2 0:00.10 sendmail
26bc47f6-... 15125 smmsp   20 0 78228 2036  584 S 0.0  0.2 0:00.00 sendmail
26bc47f6-... 15134 root  20 0  175264 3612 1884 S 0.0  0.4 0:00.15 httpd
26bc47f6-... 15139 48    20 0  175264 2220  476 S 0.0  0.2 0:00.00 httpd
26bc47f6-... 15144 root  20 0 20028 1208  612 S 0.0  0.1 0:00.01 crond
26bc47f6-... 15151 root  20 0  4116  592  512 S 0.0  0.1 0:00.00 mingetty
26bc47f6-... 15152 root  20 0  4116  596  512 S 0.0  0.1 0:00.00 mingetty
----

As you can see, `vztop` provides an ongoing look at the processor activity in real time (the display is updated every 5 seconds by default, but you can change that with the `d` command-line option or by pressing *s*. It displays a list of the most CPU-intensive tasks on the system and can provide an interactive interface for manipulating processes. It can sort the tasks by CPU usage, memory usage, and runtime. Specifying MyCT1 after the `-E` option allows you to display only those processes that are running inside container MyCT1 only. Besides, most features can be selected by an interactive command, for example, `e` and `E` described above.

Determining Container UUIDs by Process IDs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Each process is identified by a unique PID (process identifier), which is the entry of that process in the kernel's process table. For example, when you start Apache, it is assigned a process ID. This PID is then used to monitor and control this program. The PID is always a positive integer. In Virtuozzo, you can use the `vzpid` (retrieve process ID) utility to print the container UUID the process with the given id belongs to. Multiple process IDs can be specified as arguments. In this case the utility will print the container number for each of the processes.

The typical output of the `vzpid` utility is shown below:

[subs="quotes"]
----
*# vzpid 12*
Pid               VEID    Name
14663     26bc47f6-...    init
----

In our example the process with the identifier 14663 has the name `init` and is running in the container with UUID 26bc47f6-{skipped}.

[NOTE]
[subs="quotes"]
====
*Note:* You can also display the container UUID where the corresponding process is running by using the `vzps` utility.
====

Managing Virtuozzo Network
--------------------------

The given chapter familiarizes you with the Virtuozzo network structure, lists networking components, and explains how to manage these components in your working environments. In particular, it provides the following information:

[options="compact"]
* How you can manage network adapters on the hardware node.
* What virtual networks are and how you can manage them on the hardware node.
* How to create virtual network adapters inside your containers and configure their parameters.
* How to connect containers to different networks.

Managing Network Adapters on the hardware node
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Network adapters installed on the hardware node are used to provide containers with access to each other and to external networks. During the installation, Virtuozzo registers all physical and VLAN network adapters available on the server. In addition to that, it creates a number of VLAN adapters on the server. Once Virtuozzo has been successfully installed, you can do the following:

[options="compact"]
* Manage network adapters using standard Linux tools, e.g., the `ip` command.
* Create new VLAN adapters on the server.

[NOTE]
[subs="quotes"]
====
*Note:* For more information on Virtual Networks, refer to <<_managing_virtual_networks>>.
====

These operations are described in the following subsections in detail.

Networking Modes in Virtuozzo
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Container Network Modes
^^^^^^^^^^^^^^^^^^^^^^^

In Virtuozzo, any container can operate in one of the two networking modes: host-routed or bridged.

[NOTE]
[subs="quotes"]
====
*Note:* IPSec connections inside containers are supported.
====

Host-Routed Mode for Containers
+++++++++++++++++++++++++++++++

By default, a new container starts operating in the host-routed mode. In this mode, the container uses a special network adapter, `venet0`, to communicate with the server where it resides, with the other containers on the server, and with computers on external networks. The figure below demonstrates an example network configuration where all containers are set to work in the host-routed mode.

image::images/container_venet_mode.png[align="center"]

In this configuration:

* Containers #1, #2, and #3 use the `venet0` adapter as the default gateway to send and receive data to/from other networks. They also use this adapter to exchange the traffic between themselves.
* When containers #1, #2, and #3 start, the server creates ARP and routing entries for them in its ARP and routing tables. You can view the current ARP and routing entries on a server using the `arp -n` and `route -n` commands. For example:
+
[subs="quotes"]
----
*# arp -n*
Address              HWtype   HWaddress            Flags Mask      Iface
10.30.0.4            ether    00:1a:e2:c7:17:c1    C               enp0s5
10.30.23.162         ether    70:71:bc:42:f6:a0    C               enp0s5
192.168.200.101      *        *                    MP              enp0s5
192.168.200.102      *        *                    MP              enp0s5
192.168.200.103      *        *                    MP              enp0s5
*# route -n*
Kernel IP routing table
Destination       Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.200.101   *               255.255.255.255 UH    1000   0        0 venet0
192.168.200.102   *               255.255.255.255 UH    1000   0        0 venet0
192.168.200.103   *               255.255.255.255 UH    1000   0        0 venet0
10.30.0.0         *               255.255.0.0     U     0      0        0 enp0s5
default           parallels.com   0.0.0.0         UG    0      0        0 enp0s5
----
+
As you can see, the ARP and routing tables contain entries about IP addresses 192.168.200.101, 192.168.200.102, and 192.168.200.103 that belong to containers #1, #2, and 3#.

* All container outgoing network traffic goes to the `venet0` adapter and is forwarded via the `enp0s5` physical adapter to the destination, according to the routing table of the server.
* All container incoming network traffic is also processed by the `venet0` adapter. Consider the following situation:
+
--
[options="compact"]
. *Computer X* on the local network wants to send a data packet to container #1 with IP address 192.168.200.101, so it issues an ARP request which computer has this IP address.
. The server hosting container #1 replies with its MAC address.
. *Computer X* sends the data packet to the indicated MAC address.
. The server receives the packet and transmits it to `venet0` that forwards the packet to container #1.
--

Bridged Mode for Containers
+++++++++++++++++++++++++++

The default network adapter of a container can operate in the host-routed mode only. You can, however, create additional virtual adapters in containers and make them operate in the bridged network mode. The following figure shows an example network configuration where containers #1 and #2 are set to work in the bridged mode.

image::images/veth_mode_cts.png[align="center"]

In this configuration:

* Container #1 and container #2 have separate virtual adapters consisting of two network interfaces:
+
--
[options="compact"]
* An +enp0s__<X>__+ interface in the container (*enp0s5* in the figure). This interface represents a counterpart of a physical network adapter installed on a standalone server. Like any other physical adapter, it has a MAC address, can be assigned one or more IP addresses, included in different networks, and so on.
* A +veth__<X>__+ interface on the Hardware Node (*veth101.0* and *veth102.0* in the figure). This interface is mostly used to maintain the communication between the Hardware Node and Ethernet interfaces in containers.
--
+
[NOTE]
[subs="quotes"]
====
*Note:* To simplify things, virtual adapters operating in the bridged mode are called `veth` adapters, though it is not quite correct from the technical point of view.
====
+
Both interfaces are closely linked to each other, so a data packet entering one interface always comes out from the other one.

* Containers #1 and #2 keep their own ARP and routing tables that they consult when sending or receiving data.
* The `veth` adapters of both containers are bridged through the bridge `virbr1` to the physical network adapter `enp0s5`.
* All container outgoing traffic comes via the `veth` adapters to the bridge and are then transmitted through the `enp0s5` physical adapter to the destination, according to the routing tables stored in the containers.
* All incoming data packets for container #1 and #2 reach the `enp0s5` physical adapter first and are then sent through the bridge to the `veth` adapter of the destination container.

Differences Between Host-Routed and Bridged Network Modes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The bridged network mode demonstrates a number of differences as compared to the host-routed one:

[options="compact"]
* Each `vme` or `veth` virtual adapter has a MAC address assigned to it while a host-routed adapter does not have any. Thanks to this fact:
+
--
[options="compact"]
* Any container can see all broadcast and multicast packets received from or sent to the selected network adapter on the Hardware Node.
* Using bridged virtual adapters, you can host DHCP or Samba servers in containers.
--
* There is no more need to assign all network settings (IP addresses, subnet mask, gateway, and so on) to containers from the server. All network parameters can be set from inside containers.
* `veth` and `vme` adapters can be bridged among themselves and with other devices. If several `veth` and `vme` adapters are united into a bridge, this bridge can be used to handle network traffic for the containers whose `veth` and `vme` adapters are included in the bridge.
* Due to the fact that `veth` and `vme` adapters act as full members on the network (rather than 'hidden' beyond virtual networks adapters on the server), they are more prone to security vulnerabilities: traffic sniffing, IP address collisions, and so on. Therefore, `veth` and `vme` adapters are recommended for use in trusted network environments only.

Configuring Containers in Host-Routed Mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can configure the following parameters of network adapters that operate in the host-routed mode:

[options="compact"]
* IP addresses and network masks
* DNS servers
* DNS search domains

Setting IP Addresses
^^^^^^^^^^^^^^^^^^^^

The session below shows how to set IP addresses for the container MyCT1

[subs="quotes"]
----
*# prlctl set MyCT1 --ipadd 10.0.186.101/24*
*# prlctl set MyCT1 --ipadd fe80::20c:29ff:fe01:fb08*
----

For container MyCT1, you do not need to specify the network card name; `prlctl set` automatically performs the operation on the default adapter that always operates in the host-routed mode.

Setting DNS Server Addresses
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To set a DNS server for the container MyCT1, you can use the following command.

[subs="quotes"]
----
*# prlctl set MyCT1 --nameserver 192.168.1.165*
----

Setting DNS Search Domains
^^^^^^^^^^^^^^^^^^^^^^^^^^

To set a DNS search domain for the container MyCT1, run this command:

[subs="quotes"]
----
*# prlctl set MyCT1 --searchdomain 192.168.10.10*
----

[NOTE]
[subs="quotes"]
====
*Notes:*
[options="compact"]
. Network adapters operating in the routed mode must have at least one static IP address assigned.
. To assign network masks to containers operating in the `venet0` networking mode, you must set the `USE_VENET_MASK` parameter in the `/etc/vz/vz.conf` configuration file to `yes`.
. Containers can have only one network adapter operating in the host-routed mode.
====

Configuring Containers in Bridged Mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This section describes all operations related to configuring containers that operate in bridged mode.

Managing Virtual Networks
^^^^^^^^^^^^^^^^^^^^^^^^^

A virtual network acts as a binding interface between a virtual network adapter in a container and the corresponding network adapter on the hardware node. Using virtual networks, you can include containers in different networks. Virtuozzo enables you to manage virtual networks as follows:

[options="compact"]
* Create virtual networks.
* Configure virtual network parameters.
* List existing virtual networks.
* Delete virtual networks.

These operations are described in the following subsections in detail.

Creating Virtual Networks
+++++++++++++++++++++++++

By default, Virtuozzo creates the following virtual networks on the server:

[options="compact"]
* *Bridged* virtual network that is connected to one of the physical adapters on the hardware node (as a rule, `enp0s5`) and provides containers included in this virtual network with access to the network behind this physical adapter.
* *Host-only* virtual network that is connected to a special virtual adapter on the server and allows the containers joined to this virtual network to access only the server and the other containers on this network.

You can create your own virtual networks using the `prlsrvctl` command. For example, to create a new virtual network `network1`, you can run:

[subs="quotes"]
----
*# prlsrvctl net add network1*
----

By default, the command creates a host-only virtual network, but you can change its type if needed (see <<_configuring_virtual_network_parameters>>).

*Viewing Bridges*

A virtual network is associated with a bridge that is automatically made on the hardware node when you create the virtual network and serves as the basis for virtual network operation. To find out what bridge is associated with what virtual network, you can run the following command:

[subs="quotes"]
----
*# brctl show*
bridge name     bridge id               STP enabled     interfaces
br0             8000.001c42a1f380       no              enp0s5
virbr1          8000.52540021ba11       no              virbr1-nic
----

Configuring Virtual Network Parameters
++++++++++++++++++++++++++++++++++++++

Virtuozzo allows you to configure the following parameters for a virtual network:

[options="compact"]
* the name assigned to the virtual network
* the networking mode in which the virtual network is operating
* the description of the virtual network

All these operations can be performed using the `prlsrvctl` utility. Let us assume that you want to configure the `network1` virtual network. This virtual network is currently configured as a host-only network and has the following description: `This is a host-only virtual network`. To change these parameters, you can execute the following command:

[subs="quotes"]
----
*# prlsrvctl net set network1 -n network2 -t bridged --ifname enp0s6 -d "This is now a \*
*bridged virtual network"*
----

This command configured the `network1` virtual network as follows:

[options="compact"]
. Changes the virtual network name to `network2`.
. Changes the virtual network type to bridged.
. Changes the virtual network description to the following: "This is now a bridged virtual network".

Listing Virtual Networks
++++++++++++++++++++++++

To list the virtual networks existing on the hardware node, you can use the `prlsrvctl` utility as shown below.

[subs="quotes"]
----
*# prlsrvctl net list*
Network ID        Type      Bound To       Bridge
Host-Only         host-only                virbr1
Bridged           bridged   enp0s5         br0
----

This utility displays the following information on virtual networks:

[options="header",cols="1,3"]
|====
|Column|Description

|Network ID|The name assigned to the virtual network.
|Type|The networking mode set for the virtual network.
|Bound To|The adapter on the hardware node connected to the virtual networks, if any.
|====

Connecting Virtual Networks to Adapters
+++++++++++++++++++++++++++++++++++++++

By connecting an adapter on the physical server to a virtual network, you can join all containers included in the virtual network to the network to which the corresponding adapter is connected.

Let us assume the following:

[options="compact"]
* The `enp0s6` physical adapter and the `pcsnet1` virtual network exist on the hardware node. For information on creating virtual networks, see <<_creating_a_virtual_network>>.
* The `enp0s6` physical adapter is connected to the local network.
* The container MyCT1 is connected to the `pcsnet1` virtual network. Detailed information on joining containers to virtual networks is given in <<_connecting_containers_to_virtual_networks>>.

To connect the `enp0s6` adapter to the `network1` virtual network and thus to join the container MyCT1 to the network behind `enp0s6`, run this command on the server:

[subs="quotes"]
----
*# vznetcfg net addif network1 enp0s6*
----

To check that the `enp0s6` physical adapter has been successfully added to the `network1` virtual network, you can execute the following command:

[subs="quotes"]
----
*# vznetcfg if list*
Name     Type   Network ID   Addresses
enp0s6     nic    pcsnet1     10.31.252.116/16,fe80::2a9:40ff:fe0f:b6f2/64,dhcp
...
----

As you can see, the `enp0s6` adapter is now joined to the `pcsnet1` virtual network. That means that the container MyCT1 whose virtual network adapter is connected to `pcsnet1` can access the local network behind `enp0s6`.

At any time, you can disconnect the `enp0s6` physical adapter from the `pcsnet1` virtual network (and thus detach the container MyCT1 from the local network) by running the following command:

[subs="quotes"]
----
*# vznetcfg net delif enp0s6*
----

Deleting Virtual Networks
+++++++++++++++++++++++++

At any time, you can remove a virtual network that you do not need any more from the physical server. To do this, you can use both the `vznetcfg` and `prlsrvctl` utilities. For example, you can delete the `pcsnet1` virtual network by running the following command:

[subs="quotes"]
----
*# prlsrvctl net del pcsnet1*
----

To check that `pcsnet1` has been successfully removed, execute this command:

[subs="quotes"]
----
*# prlsrvctl net list*
Network ID     Type        Bound To
Host-Only      host-only   
Bridged        bridged     enp0s5
----

Managing Virtual Network Adapters in Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Virtuozzo provides you with ample opportunities of configuring `veth` virtual network adapters in containers and including them in different network environments. This section shows you the way to perform the following operations:

[options="compact"]
* Create new virtual network adapters in containers and delete existing ones.
* Configure the parameters of an existing virtual network adapter.
* Join container virtual network adapters to virtual networks.

All these operations are described in the following subsections in detail.

Creating and Deleting veth Network Adapters
+++++++++++++++++++++++++++++++++++++++++++

By default, any container on the hardware node starts functioning in the `venet0` mode right after its creation. However, at any time you can create additional virtual adapters for containers and set them to work in the bridged mode. You can do this using the `--netif_add` option of the `prlctl set` command.

Let us assume that you wish to create a new virtual adapter with the name of `enp0s6` in container MyCT1 and make it function in the bridged mode. To do this, run the following command :

[subs="quotes"]
----
*# prlctl set MyCT1 --netif_add enp0s6*
----

The settings of the newly created virtual adapter are saved as the value of the `NETIF` parameter in the configuration file of container MyCT1 (`/etc/vz/conf/26bc47f6-353f-444b-bc35-b634a88dbbcc.conf`). So, you can use the following command to display the parameters assigned to the `veth` network adapter in container MyCT1:

[subs="quotes"]
----
*# grep NETIF /etc/vz/conf/26bc47f6-353f-444b-bc35-b634a88dbbcc.conf*
NETIF="ifname=enp0s6,mac=00:10:41:F0:AA:B6,host_mac=00:18:51:A0:8A:D7"
----

As you can see, the parameters set for the `veth` virtual network adapter during its creation are the following:

[options="compact"]
* `ifname`, name set for the `veth` Ethernet interface in container MyCT1. You specified this name when creating the container virtual network adapter. Usually, names of Ethernet interfaces in containers are set in the form of +eth_N_+ where _N_ denotes the index number of the created adapter (for example, `enp0s5` or `enp0s6`). However, you can choose any other name you like and specify it during the virtual adapter creation.
* `mac`, MAC address assigned to the `veth` Ethernet interface in container MyCT1.
* `host_mac`, MAC address assigned to the `veth` Ethernet interface on the hardware node.

`ifname` is the only mandatory parameter that you need to specify when creating a container virtual network adapter. All the other parameters are optional and generated by Virtuozzo automatically, if not indicated.

At any time, you can remove the `veth` virtual network adapter from container MyCT1 by executing the following command:

[subs="quotes"]
----
*# prlctl set MyCT1 --netif_del enp0s6*
----

Configuring veth Adapter Parameters
+++++++++++++++++++++++++++++++++++

While functioning in the bridged mode, each container virtual network adapter appears as a full participant on the network to which it is connected and needs to have its own identity on this network.

Fist of all, to start functioning on a TCP/IP network, a `veth` virtual adapter should be assigned an IP address. This can be done as follows:

[subs="quotes"]
----
*# prlctl set MyCT1 --ifname enp0s6 --ipadd 192.168.144.123*
----

This command sets an IP address of `192.168.144.123` for the `enp0s6` adapter in container MyCT1. If you want to use the Dynamic Host Configuration Protocol (DHCP) to make the `enp0s6` adapter of container MyCT1 automatically receive TCP/IP configuration settings, you can issue the following command instead:

[subs="quotes"]
----
*# prlctl set MyCT1 --ifname enp0s6 --dhcp yes*
----

Any static IP address assigned to the container virtual network adapter can be removed by executing the following command:

[subs="quotes"]
----
*# prlctl set MyCT1 --ifname enp0s6 --ipdel 192.168.144.123*
----

You can also delete all IP addresses set for container MyCT1 at once:

[subs="quotes"]
----
*# prlctl set MyCT1 --ifname enp0s6 --ipdel all*
----

You may also wish to set the following parameters for a container network adapter:

* A DNS server that the container virtual adapter is supposed to use:
+
[subs="quotes"]
----
*# prlctl set MyCT1 --ifname enp0s6 --nameserver 192.168.100.111*
----
* A gateway to be used for routing the traffic of the container virtual adapter:
+
[subs="quotes"]
----
*# prlctl set MyCT1 --ifname enp0s6 --gw 192.168.111.1*
----

Connecting Containers to Virtual Networks
+++++++++++++++++++++++++++++++++++++++++

With the implementation of `veth` virtual adapters allowing containers to function as full participants on the network, it has become possible to include containers in a wide range of network configurations the most common of which are Ethernet networks and VLANs (virtual local area networks). The process of connecting `veth` virtual network adapters to an Ethernet network or to a VLAN is carried out using certain physical and VLAN adapters, respectively, available on the server and involves completing the following tasks:

[options="compact"]
. Creating a virtual network that will act as an intermediary between the `veth` adapters and the physical/VLAN adapter.
. Connecting the `veth` virtual adapter you want to include in an Ethernet network/VLAN to the virtual network.
. Joining the virtual network where the `veth` virtual adapters are included to the corresponding physical/VLAN adapter.

After completing these tasks, the container virtual network adapters will be able to communicate with any computer on the network (either Ethernet or VLAN) where they are included and have no direct access to the computers joined to other networks.

For details on creating new virtual networks and joining physical and VLAN adapters to them, see <<_creating_a_virtual_network>> and <<_connecting_virtual_networks_to_adapters>>, respectively. In the example below we assume the following:

[options="compact"]
* The `enp0s5` physical adapter and the `pcsnet1` virtual network exist on the server.
* The `enp0s5` physical adapter is connected to the local Ethernet network and to the `pcsnet1` virtual network.
* You want to connect container MyCT1 and container MyCT2 to the local Ethernet network.

To join container MyCT1 and MyCT2 to the local Ethernet network behind the `enp0s5` adapter, you need connect these containers to the `pcsnet1` virtual network. To do this:

. Find out the name of the `veth` Ethernet interfaces in container MyCT1 and MyCT2:
+
[subs="quotes"]
----
*# vzlist -a -o ctid,ifname*
                                CTID IFNAME
26bc47f6-353f-444b-bc35-b634a88dbbcc enp0s5
d59da013-4cb9-46e0-9342-a4f803db4078 enp0s6
da72a150-94c9-4075-a6a4-e53a3b3147fc -
----
+
The command output shows that the `veth` Ethernet interfaces in container MyCT1 and MyCT2 have the names of `enp0s6` and `enp0s5`, respectively.
+
[NOTE]
[subs="quotes"]
====
*Note:* To add a `veth` adapter to a virtual network, you must use the name of its Ethernet interface in the container. 
====
. Join the `veth` adapters to the `pcsnet1` virtual network:
** Add the `veth` adapter of container MyCT1 to the virtual network:
+
[subs="quotes"]
----
*# prlctl set MyCT1 --ifname enp0s6 --network pcsnet1*
----
** Add the `veth` adapter of container MyCT2 to the virtual network:
+
[subs="quotes"]
----
*# prlctl set MyCT2 --ifname enp0s5 --network pcsnet1*
----

After completing these tasks, containers MyCT1 and MyCT2 will be able to access any of the servers in the network where the `enp0s5` physical adapter is connected.

At any time, you can disconnect the `veth` virtual network adapters of containers MyCT1 and MyCT2 from the `pcsnet1` virtual network by executing the following commands:

* To disconnect the `veth` adapter of container MyCT1 from the virtual network:
+
[subs="quotes"]
----
*# prlctl set MyCT1 --ifname enp0s6 --network ""*
----
* To disconnect the `veth` adapter of container MyCT2 from the virtual network:
+
[subs="quotes"]
----
*# prlctl set MyCT2 --ifname enp0s6 --network ""*
----

Keeping Your System Up To Date
------------------------------

This chapter explains the ways to keep your hardware node up to date. The components you need to take care of are the following:

[options="compact"]
* Virtuozzo software
* containers hosted on the server

Updating Virtuozzo
~~~~~~~~~~~~~~~~~~

Virtuozzo allows quick and easy updates with the `yum` utility standard for RPM-compatible Linux operating systems. The main components you may need to update are the following:

[options="compact"]
* utilities and libraries,
* kernel,
* EZ templates.

Updating All Components
^^^^^^^^^^^^^^^^^^^^^^^

The easiest way to update all components of the Virtuozzo software is to simply run the `yum update` command. When executed, this command tells the `yum` utility to do the following:

[options="compact"]
. Access remote Odin repositories.
. Check for available updates for the Virtuozzo kernel, utilities, libraries, and EZ templates.
. Install the found updates on your system.

Note that the `yum` utility can only update the packages that are already installed on the server. So if a package is not available on your system, you first need to install the package using the `yum install` command.

Updating Kernel
^^^^^^^^^^^^^^^

Updating the Virtuozzo kernel requires updating the following packages: `vzkernel`, `vzkernel-devel`:

[subs="quotes"]
----
*# yum update vzkernel vzkernel-devel*
----

Updating EZ Templates
^^^^^^^^^^^^^^^^^^^^^

You can update an EZ template like any other RPM package using the `yum update` command. For example:

[subs="quotes"]
----
*# yum update centos-6-x86-ez*
...
Updated:
  centos-6-x86-ez.noarch 0:4.7.0-1                                                                                                                                      
Complete!
----

[NOTE]
[subs="quotes"]
====
*Notes:*
[options="compact"]
. Updating an OS EZ template requires that you append `ez` to template name.
. You can also use the `vzpkg update template` command to update EZ templates.
====

Checking for Updates
^^^^^^^^^^^^^^^^^^^^

Before updating any packages, you may want to see which can be updated and to what version. You can do that with the `yum check-update` command. For example:

[subs="quotes"]
----
*# yum check-update*
...
vzkernel.x86_64                   2.6.32-042stab049.5             pcs-base
vzkernel-devel.x86_64             2.6.32-042stab049.5             pcs-base
----

Performing More Actions with yum
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The `yum` command allows you to do more than just check for and install updates. Some of the other useful options that may help you in updating Virtuozzo are `search`, `list`, `info`, `deplist`, `provide`. For more information on these and other options, see the `yum` manual page.

Updating Containers
~~~~~~~~~~~~~~~~~~~

Virtuozzo provides two means of keeping your containers up to date:

[options="compact"]
* Updating EZ templates software packages inside a particular container by means of the `vzpkg` utility. Using this facility, you can keep any of the containers existing on your hardware node up to date.
* Updating caches of the OS EZ templates installed on the hardware node. This facility allows you to create new containers already having the latest software packages installed.

Updating EZ Template Packages in Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Virtuozzo allows you to update packages of the OS EZ template a container is based on and of any application EZ templates applied to the container. You can do it by using the `vzpkg update` utility. Assuming that container MyCT1 is based on the `redhat-el5-x86` OS EZ template, you can issue the following command to update all packages included in this template:

[subs="quotes"]
----
*# vzpkg update 26bc47f6-353f-444b-bc35-b634a88dbbcc redhat-el5-x86*
...
  Updating: httpd                ####################### [1/4]
  Updating: vzdev                ####################### [2/4]
  Cleanup : vzdev                ####################### [3/4]
  Cleanup : httpd                ####################### [4/4]
Updated: httpd.i386 0:2.0.54-10.2 vzdev.noarch 0:1.0-4.swsoft
Complete!
Updated:
 httpd                  i386      0:2.0.54-10.2
 vzdev                  noarch    0:1.0-4.swsoft
----

[NOTE]
[subs="quotes"]
====
*Notes:*
[options="compact"]
. Updating EZ templates is supported for running containers only.
. If  you are going to update the cache of a commercial OS EZ template (e.g., Red Hat Enterprise Server 5 or SLES 10), you should first update software packages in the remote repository used to handle this OS EZ template and then proceed with updating the EZ template cache.
====

As you can see from the example above, the `httpd` and `vzdev` applications have been updated for the `redhat-el5-x86` OS EZ template. If you wish to update all EZ templates (including the OS EZ template) inside container MyCT1 at once, execute this command:

[subs="quotes"]
----
*# vzpkg update 26bc47f6-353f-444b-bc35-b634a88dbbcc*
...
Running Transaction
  Updating  : hwdata               ###################### [1/2]
  Cleanup   : hwdata               ###################### [2/2]
Updated: hwdata.noarch 0:1.0-3.swsoft
Complete!
Updated:
 hwdata                 noarch    0:0.158.1-1
----

In the example above, only the `hwdata` package inside container MyCT1 was out of date and updated to the latest version.

Updating OS EZ Template Caches
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

With the release of new updates for the corresponding Linux distribution, the created OS EZ template cache can become obsolete. Virtuozzo allows you to quickly update your OS EZ template caches using the `vzpkg update cache` command.

[NOTE]
[subs="quotes"]
====
*Note:* If  you are going to update the cache of a commercial OS EZ template (e.g., Red Hat Enterprise Server 6 or SLES 11), you should first update software packages in the remote repository used to handle this OS EZ template and then proceed with updating the EZ template cache.
====

When executed, `vzpkg update cache` checks the `cache` directory in the template area (`/vz/template/cache` by default) on the hardware node and updates all existing tarballs in this directory. However, you can explicitly indicate the tarball for what OS EZ template should be updated by specifying the OS EZ template name. For example, to update the tarball for the `centos-6-x86` OS EZ template, run this command:

[subs="quotes"]
----
*# vzpkg update cache centos-6-x86*
Loading "rpm2vzrpm" plugin
Setting up Update Process
Setting up repositories
base0            100% |=========================|  951 B    00:00
base1            100% |=========================|  951 B    00:00
base2            100% |=========================|  951 B    00:00
base3            100% |=========================|  951 B    00:00
...
----

Upon the `vzpkg update cache` execution, the old tarball name gets the `-old` suffix (e.g., `centos-x86.tar.gz-old`):

You can also pass the `-f` option to `vzpkg update cache` to remove an existing tar archive and create a new one instead of it.

If the `vzpkg update cache` command does not find a tarball for one or several OS EZ templates installed on the server, it creates tar archives of the corresponding OS EZ templates and puts them to the `/vz/template/cache` directory.

Advanced Tasks
--------------

This chapter describes those tasks that are intended for advanced system administrators who would like to obtain deeper knowledge about Virtuozzo capabilities.

Configuring Capabilities
~~~~~~~~~~~~~~~~~~~~~~~~

Capabilities are sets of bits that permit of splitting the privileges typically held by the root user into a larger set of more specific privileges. The POSIX capabilities are defined by a draft IEEE standard (IEEE Std 1003.1e); they are not unique to Linux or Virtuozzo. When the Linux or Virtuozzo documentation says "requires root privileges", in nearly all cases it really means "requires a specific capability".

This section documents the tasks that can be achieved using per-container capabilities in Virtuozzo and all configurable capabilities.

Available Capabilities for Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This section lists all the capabilities that can be set with the `vzctl set` command. The capabilities are divided into two tables: the capabilities defined by the POSIX draft standard and Linux-specific capabilities. For each capability, its description is given together with the default value for a container.

Please note that it is easy to create a non-working container or compromise your Hardware Node security by setting capabilities incorrectly. Do not change any capability for a container without a full understanding of what this capability can lead to.

Capabilities Defined by POSIX Draft
+++++++++++++++++++++++++++++++++++

[options="header",cols="2,5,1"]
|====
|Name|Description|Default

|`chown`|If a process has this capability set on, it can change ownership on the files not belonging to it or belonging to another user. You have to set this capability on to allow the container root user to change ownership on files and directories inside the container.|on
|`dac_override`|This capability allows to access files even if the permission is set to disable access. Normally leave this on to let the container root access files even if the permission does not allow it.|on
|`dac_read_search`|Overrides restrictions on reading and searching for files and directories. The explanation is almost the same as above with the sole exclusion that this capability does not override executable restrictions.|on
|`fowner`|Overrides restrictions on setting the `S_ISUID` and `S_ISGID` bits on a file requiring that the effective user ID and effective group ID of the process shall match the file owner ID. |on
|`fsetid`|Used to decide between falling back on the old `suser()` or `fsuser()`.|on
|`kill`|Allows sending signals to processes owned by other users.|on
|`setgid`|Allows group ID manipulation and forged group IDs on socket credentials passing.|on
|`setuid`|Allows user ID manipulation and forged user IDs on socket credentials passing.|on
|====

Linux-specific Capabilities
+++++++++++++++++++++++++++

[options="header",cols="2,5,1"]
|====
|Name|Description|Default
|`setpcap`|Transfer any capability in your permitted set to any process ID; remove any capability in your permitted set from any process ID.|off
|`linux_immutable`|Allows the modification of the `S_IMMUTABLE` and `S_APPEND` file attributes. These attributes are implemented only for the EXT2FS and EXT3FS Linux file systems. However, if you bind mount a directory located on the EXT2FS or EXT3FS file system into a container and revoke this capability, the root user inside the container will not be able to delete or truncate files with these attributes on.|on
|`net_bind_service`|Allows to bind to sockets with numbers below 1024.|on
|`net_broadcast`|Allows network broadcasting and multicast access.|on
|`net_admin`|Allows the administration of IP firewalls and accounting.|off
|`net_raw`|Allows to use the RAW and PACKET sockets.|on
|`ipc_lock`|Allows to lock shared memory segments and `mlock/mlockall` calls.|on
|`ipc_owner`|Overrides IPC ownership checks.|on
|`sys_module`|Insert and remove kernel modules. Be very careful with setting this capability on for a container; if a user has the permission of inserting kernel modules, this user has essentially full control over the Hardware Node.|off
|`sys_chroot`|Allows to use `chroot()`.|on
|`sys_ptrace`|Allows to trace any process.|on
|`sys_pacct`|Allows to configure process accounting.|on
|`sys_admin`|In charge of many system administrator tasks such as swapping, administering APM BIOS, and so on. Shall be set to off for containers.|off
|`sys_boot`|This capability currently has no effect on the container behaviour.|on
|`sys_nice`|Allows to raise priority and to set priority for other processes.|on
|`sys_resource`|Override resource limits (do not confuse with user beancounters).|on
|`sys_time`|Allows to change the system time.|off
|`sys_tty_config`|Allows to configure TTY devices.|on
|`mknod`|Allows the privileged aspects of `mknod()`.|on
|`lease`|Allows to take leases of files.|on
|====

Creating Customized Containers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you wish to use one or more custom applications in many identical containers, you may want to create containers with necessary applications already preinstalled and tuned to meet your demands.

Virtuozzo offers several ways to create customized containers with preinstalled applications:

[options="compact"]
* By creating an OS EZ template cache with preinstalled application templates.
* By making a customized base OS EZ template and using it as the basis for containers.
* By making a non-base OS EZ template and using it as the basis for containers.
* By making a customized application EZ template, adding it to a new configuration sample file, and using this sample file as the basis for containers.

All these operations are described in the following subsections in detail.

Using OS Template Caches with Preinstalled Application Templates
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can use an OS EZ template cache with preinstalled application templates to quickly create multiple identical containers without having to install applications manually or wait until they are installed automatically to each container after its creation. The best way to create such a cache is:

. Make a custom sample configuration file with information on the OS EZ template to cache and application EZ templates to preinstall. For example:
+
[subs="quotes"]
----
*# cp /etc/vz/conf/ve-basic.conf-sample \*
     */etc/vz/conf/ve-centos-6-x86-mysql-devel.conf-sample*
----
+
[NOTE]
[subs="quotes"]
====
*Note:* If you already have a custom sample configuration file with application EZ templates specified in it, you can reuse it instead of creating a new one.
====
. Add the OS EZ template and application EZ template information to the new configuration file. Each OS and application template name must be preceded by a dot. Multiple consecutive application EZ template names must be separated by white spaces. For example:
+
[subs="quotes"]
----
*# cd /etc/vz/conf*
*# echo 'OSTEMPLATE=".centos-6-x86"' &gt;&gt; ve-centos-6-x86-mysql-devel.conf-sample*
*# echo 'TEMPLATES=".mysql .devel"' &gt;&gt; ve-centos-6-x86-mysql-devel.conf-sample*
----

. Run the `vzpkg create appcache` command with your configuration file as an option. For example:
+
[subs="quotes"]
----
*# vzpkg create appcache --config centos-6-x86-mysql-devel*
----
+
[NOTE]
[subs="quotes"]
====
*Note:* If the resulting cache already exists, it will not be recreated and you will see a corresponding message.
====

The resulting archive can be found in the `/vz/template/cache` directory on the Hardware Node. You can check that it exists and includes necessary application templates with the following command:

[subs="quotes"]
----
*# vzpkg list appcache*
centos-6-x86                 2012-07-20 16:51:36
     mysql
     devel
----

Disabling Golden Image Functionality
++++++++++++++++++++++++++++++++++++

The Golden Image functionality allows you to preinstall application templates to OS EZ template caches to speed up creating multiple containers based on the same set of OS and application templates. Previously, you could either install application templates to each container after creating it or embed them directly into a custom OS template. Golden Image is currently the easiest and fastest way to create containers with preinstalled applications.

The Golden Image functionality is enabled by default in the `/etc/sysconfig/vz/vz.conf` global configuration file. Should you wish to disable it, do one of the following:

[options="compact"]
* Set the `GOLDEN_IMAGE` option to `no` in the Virtuozzo global configuration file. The Golden Image functionality will be disabled globally.
* Set the `GOLDEN_IMAGE` option to `no` in the container sample configuration file. The Golden Image functionality will be disabled for commands that use this specific sample configuration file.
* Create a file named `golden_image` containing `no` in the OS EZ template's configuration directory. The Golden Image functionality will be disabled for this specific OS EZ template.
* Create a file named `golden_image` containing `no` in the application template's configuration directory. The Golden Image functionality will be disabled for this specific application template, so it will not be preinstalled into any OS EZ template caches.

Using Customized OS EZ Templates
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can make a customized base OS EZ template which can then be used to create containers with a set of application already tuned to meet your demands. To make such a template, do the following:

. Create a metafile that will serve as the basis for your customized base OS EZ template.
+
[NOTE]
[subs="quotes"]
====
*Note:* While creating a metafile for a new OS EZ template, make sure that the value of either the `%osname` parameter or the `%version` parameter in the metafile differs from the names or versions of all base OS EZ templates installed on the Hardware Node.
====
. Create one or more scripts that will be executed on different stages of the OS EZ template life cycle and customize your applications to meet your needs. For example, you can create a postinstall script with the name of `post_install.bash` and make it perform a number of customization operations on some application included in the OS EZ template after installing this application inside your container.
. Create a customized OS EZ template by running the `vzmktmpl` utility and passing the corresponding options to it. So, you can use the `--post-install` option and specify the path to the `post_install.bash` script from the example above to make an OS EZ template that will customize your application after installing it inside your container.
. Install the customized OS EZ template on the Hardware Node using the `rpm -i` command.
. Cache the created OS EZ template by running the `vzpkg create cache` command.
. Create a container based on the OS EZ template.

For example, to create a container that will run CentOS 5 and have the customized `mysql` and `apache` applications installed right after its creation, you can do the following:

. Create a metafile for the CentOS EZ template, e.g., `centos_5_customized.metafile`, and save in the `/root/centos_5` directory on the Hardware Node.
. Make a script that will perform a number of custom operations after applying the `mysql` and `apache` application EZ templates to the container, and name it `post_install.bash`.
. Copy the script to the `/root/centos_5` directory on the Hardware Node.
. Execute the following command on the Hardware Node to create the CentOS 5 OS EZ template:
+
[subs="quotes"]
----
*# vzmktmpl /root/centos_5/centos_5_customized.metafile --post-install \*
           */root/centos5/post_install.bash*
----
+
This command creates an OS EZ template for CentOS and put it to the `/root` directory (for example, `/root/centos_customized-5-x86-ez-4.7.0-1.noarch.rpm`).

. Install the resulting OS EZ template on the Hardware Node:
+
[subs="quotes"]
----
*# rpm -i /root/centos_customized-5-x86-ez-4.7.0-1.noarch.rpm*
----
. Cache the installed OS EZ template:
+
[subs="quotes"]
----
*# vzpkg create cache centos_customized-5-x86*
----
. Create container MyCT1 on the basis of the new OS EZ template:
+
[subs="quotes"]
----
*# prlctl create MyCT1 --ostemplate centos_customized-5-x86*
----

So you have just created container MyCT1 having the customized `mysql` and `apache` applications installed inside it.

Using EZ OS Template Sets
^^^^^^^^^^^^^^^^^^^^^^^^^

Another way of creating customized containers is to make a non-base OS EZ template (also known as an OS EZ template set) differing from the corresponding base OS EZ template in the number of packages included in this template. For example, if you wish a container to run CentOS 5 and to function as a Linux-based server only, you can create the `centos-5-x86-server` OS EZ template set and include only those packages in it that are needed for performing main server tasks. So, you can specify packages to be used for setting up file and print sharing and exclude all the packages for graphical interfaces (GNOME and KDE).

To create a non-base OS EZ template, do the following:

. Create a metafile that will serve as the basis for your non-base OS EZ template. Any metafile for this kind of EZ template must contain the following information:
+
--
[options="compact"]
* `%osname`, the name of the Linux distribution for which you are creating the OS EZ template set. This name must correspond to that specified in the base OS EZ template. For example, if you are creating an OS template set of the base OS EZ template for CentOS 5, set the value of this parameter to `centos`.
* `%osver`, the version of the Linux distribution specified as the value of the `%osname` parameter. This name must correspond to that specified in the base OS EZ template. For example, if you are creating an OS template set of the base OS EZ template for CentOS 5, set the value of this parameter to `5`.
* `%osarch`, the system architecture where the EZ template is to be run. This name must correspond to that specified in the base OS EZ template. For example, if you are creating an OS template set of the base OS EZ template for CentOS 5, set the value of this parameter to `x86`.
* `%setname`, the name to be assigned to your non-base OS EZ template. You can specify any name you like for your OS template set.
+
--
[options="compact"]
* This name will be added to the name of the base OS EZ template after the indication of the architecture where the OS EZ template is to be run. For example, if you are creating an OS template set of the base OS EZ template for CentOS 5 that is supposed to run on x86 platforms, the name of your non-base OS EZ template should look like +centos-5-x86-_<template_name>_-ez-1.0-1.noarch.rpm+, where _<template_name>_ is the name you specify as the value of the `%setname` parameter.
* This name will also be assigned to the directory which will store the meta data of your non-base OS EZ template after the template installation on the Hardware Node. For example, it will have the name of `/vz/template/centos/5/x86/config/os/my_non_base_template` if you set the value of this parameter to `my_non_base_template`, create a non-base OS EZ template for CentOS 5, and installed it on the Hardware Node.
--
`%packages`:: a list of RPM packages to be included in the non-base OS EZ template. This parameter allows you to specify what applications will be present inside your containers based on this OS EZ template set right after their installation. The names of the packages listed as the value of this parameter must correspond to the names of real RPM packages (without indicating the package version, release, architecture, and the `.rpm` extension) that are stored in the repository used for managing your EZ templates.
--
+
[NOTE]
[subs="quotes"]
====
*Note:* You can also specify a number of additional parameters in your metafile. For example, you may wish to add one or several extra packages to your OS EZ template set which are not available in the repository used to handle the packages for the corresponding base OS EZ template. For this purpose, you will have to specify the `%mirrorlist` parameter providing information on the repository where these extra packages are kept.
====
. You can also (though you do not have to) create a number of scripts that will be executed on different stages of the non-base OS EZ template life cycle and customize your applications to meet your demands. The path to these scripts should then be specified after the corresponding options while creating your OS template set. For example, you can create a preinstall script with the name of `pre_install.bash` and make it perform a number of customization operations on some application included in the non-base OS EZ template before installing this application in your container.
+
[NOTE]
[subs="quotes"]
====
*Note:* If there are no scripts for a non-base OS EZ template, the scripts available for the corresponding base OS EZ template will be executed.
====
. Create the non-base OS EZ template by running the `vzmktmpl` utility and passing the corresponding options to it, if needed. So, if you created one or several scripts in the previous step, you can use special options and specify the path to these scripts during the command execution. For example, you can use the `--pre-install` option and specify the path to the `pre_install.bash` script to make an OS EZ template that will customize your application before installing it inside your container.
. Install the non-base OS EZ template on the Hardware Node using the `rpm -i` command.
. Cache the created OS EZ template by running the `vzpkg create cache` command.
. Create a container based on the OS EZ template.

Using Customized Application Templates
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If the number of customized applications inside your containers is relatively small, you can also use the following way of creating customized containers:

. Create a metafile that will serve as the basis for your customized application EZ template.
. Create one or more scripts that will be executed on different stages of the application EZ template lifecycle and customize your applications to meet your demands. For example, you can create a postinstall script with the name of `post_install.bash` and make it perform a number of customization operations on your application after installing this application in your container.
. Create a customized application EZ template by running the `vzmktmpl` utility and passing the corresponding options to it. So, you can use the `--post-install` option and specify the path to the `post_install.bash` script from the example above to customize your application in accordance with your needs after installing it in your container.
+
[NOTE]
[subs="quotes"]
====
*Note:* The full list of options allowing you to specify what scripts are to be executed on what stage of the EZ template lifecycle is provided in the *vzmktmpl* section of the _Parallels Containers for Windows 6.0 Reference Guide_.
====
. Install the customized EZ template on the server using the `rpm -i` command.
. Create a new container configuration sample file and include the customized EZ template in this file. Detailed information on container configuration sample files is provided in the <<_managing_container_resources_configuration>>.
. Create a customized container on the basis of the configuration sample.

The following example demonstrates how to create container MyCT1 that will run CentOS 5 and have the customized `mysql` application installed right after its creation:

. Create a metafile for the `mysql` application, name it `mysql.metafile`, and save in the `/usr/mysql` directory on the server.
. Make a script that will perform a number of custom operations after applying the `mysql` EZ template to the container, and name it `post_install.bash`.
. Copy the script to the `/usr/mysql` directory on the server.
. Execute the following command on the server to create the `mysql` EZ template:
+
[subs="quotes"]
----
*# vzmktmpl /usr/mysql/mysql.metafile --post-install /usr/mysql/post_install.bash*
----
+
This command will create an EZ template for the `mysql` application and put it to the `/root` directory (e.g., `/root/mysql-centos-5-x86-ez-4.0.0-17.swsoft.noarch.rpm`).

. Install the `mysql` EZ template on the server. In this example, you can do it as follows:
+
[subs="quotes"]
----
*# rpm -ihv /root/mysql-centos-5-x86-ez-4.0.0-17.swsoft.noarch.rpm*
----
. Create a new container configuration sample file and add the `mysql` EZ template to a list of templates that will be installed in containers created on the basis of this configuration sample file.
. Create container MyCT1 by using the `prlctl create` command and the `mysql` sample file:
+
[subs="quotes"]
----
*# prlctl create MyCT1 --ostemplate centos-5-x86*
----

You have created container MyCT1 that already has the customized `mysql` application installed.

Creating and Configuring Docker-enabled Containers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To create a Virtuozzo container ready for running Docker containers, do the following:

. Create a container with a guest OS for which a Docker application template is available. For example:
+
[subs="quotes"]
----
*# vzctl create MyCT1 --ostemplate centos-7-x86_64*
----
+
In the current version of Virtuozzo, Docker application templates are available for CentOS 7 and Fedora 21 operating systems.

. Configure network access in the container. For details, see <<_configuring_network_settings>>.
. Enable bridges in the container. For example:
+
[subs="quotes"]
----
*# vzctl set MyCT1 --features bridge:on --save*
----
. Allow all `iptables` modules for the container. For example:
+
[subs="quotes"]
----
*# vzctl set MyCT1 --netfilter full --save*
----
. Enable `tun` device in the container. For example:
+
[subs="quotes"]
----
*# vzctl set MyCT1 --devnodes net/tun:rw --save*
----
. Start the container. For example:
+
[subs="quotes"]
----
*# vzctl start MyCT1*
----
. Install the Docker application template into the container. For example:
+
[subs="quotes"]
----
*# vzpkg install MyCT1 docker*
----

To check that Docker is configured correctly, you can create a test Docker container inside the configured Virtuozzo container. For example:

. Increase container RAM and diskspace for running multiple Docker containers. For example:
+
[subs="quotes"]
----
*# vzctl set MyCT1 --ram 4G --save*
*# vzctl set MyCT1 --diskspace 25G:25G --save*
----
. Launch MySQL:
+
[subs="quotes"]
----
*# docker run --name test-mysql -e MYSQL_ROOT_PASSWORD=123qwe -d mysql*
----
. Launch WordPress:
+
[subs="quotes"]
----
*# docker run --name test-wordpress --link test-mysql:mysql -p 8080:80 -d wordpress*
----
. Log in to your WordPress installation by visiting the IP address of the Virtuozzo container at port 8080.

[NOTE]
[subs="quotes"]
====
*Note:* For more information about Docker, visit http://docs.docker.com/.
====

Restrictions and Limitations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

[options="compact"]
* Checkpointing and live migration of Virtuozzo containers with Docker containers inside is not supported.
* Only the `vfs` Docker graph driver is currently supported.
* Bridges cannot be created inside Docker containers running inside a Virtuozzo container. (You can create bridges inside Virtuozzo containers as usual.)

Restarting Containers
~~~~~~~~~~~~~~~~~~~~~

You can restart containers from the inside using typical Linux commands, e.g., `reboot` or `shutdown -r`. Restarting is handled by the `vzeventd` daemon.

If necessary, you can forbid restarting containers from the inside as follows:

[options="compact"]
* To disable restarting for a specific container, add the `ALLOWREBOOT="no"` line to the container configuration file (+/etc/vz/conf/_&lt;UUID&gt;_.conf+).
* To disable restarting globally for all containers on the server, add the `ALLOWREBOOT="no"` line to the global configuration file (`/etc/vz/vz.conf`).
* To disable restarting globally except for specific containers, add the `ALLOWREBOOT="no"` line to the global configuration file (`/etc/vz/vz.conf`) and explicitly specify `ALLOWREBOOT="yes"` in the configuration files of the respective containers.

Enabling VNC Access to Containers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can use your favorite VNC clients to connect to and manage containers. To do this, you need to complete these steps:

[options="compact"]
. Enable VNC access in the desired container.
. Connect to the container with a VNC client.

The sections below describe both steps in details.

Enabling VNC Access to Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To enable VNC access to a container, you need to do the following:

[options="compact"]
. Make sure you have a valid user account in the container to be able to log into it.
. Make sure the container is running.
. Set the VNC mode and password for the container. For example:
+
[subs="quotes"]
----
*# prlctl set MyCT1 --vnc-mode manual --vnc-port 6501 --vnc-passwd XXXXXXXX*
----
+
[NOTE]
[subs="quotes"]
====
*Note:* Port number must be unique for each container you open VNC access to. In the auto mode, correct port numbers are assigned automatically. In the manual mode, you need to make sure port numbers are unique yourself.
====

Connecting with a VNC Client
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

After you have enabled VNC access to the container, you can connect to it with your favorite VNC client. To do this, you need to pass the following parameters to the VNC client:

[options="compact"]
* IP address of the server where the container is hosted.
* Port number and password you specified when enabling VNC access.
* Valid user account in the container.

Managing iptables Modules
~~~~~~~~~~~~~~~~~~~~~~~~~

This section describes how to manage `iptables` modules for both physical servers and containers.

Using iptables Modules in Virtuozzo
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Filtering network packets on Hardware Nodes running Virtuozzo does not differ from doing so on a standalone Linux server. You can use the standard `iptables` tool to control how network packets enter, move through, and exit the network stack within the Virtuozzo kernel.

For your reference, below are several resources you can consult to get detailed information on using `iptables` on Linux servers:

[options="compact"]
* https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Security_Guide/index.html[_Red Hat Enterprise Linux 7 Security Guide_] contains a section focusing on packet filtering basics and explaining various options available for `iptables`.
* http://www.frozentux.net/iptables-tutorial/iptables-tutorial.html[_iptables Tutorial 1.2.2_] explains in great detail how `iptables` is structured and how it works.

Defining the Basic Set of iptables Modules for Virtuozzo
++++++++++++++++++++++++++++++++++++++++++++++++++++++++

All iptables modules you plan to use must first be loaded on the Hardware Node. To do this:

. Specify the desired modules in the `IPTABLES_MODULES` parameter in the `/etc/sysconfig/iptables-config` file. For example:
+
----
IPTABLES_MODULES="ipt_REJECT iptable_filter iptable_mangle xt_length xt_hl \
xt_tcpmss xt_TCPMSS xt_multiport xt_limit xt_dscp"
----
. Restart the `iptables` service for the changes to the `/etc/sysconfig/iptables-config` file to come into effect:
+
[subs="quotes"]
----
*# service iptables restart*
----

Using conntrack Rules on Virtuozzo Hardware Nodes
+++++++++++++++++++++++++++++++++++++++++++++++++

By default, conntrack rules are disabled on the Hardware Node to save resources and increase performance when connection tracking is not needed. To enable connection tracking in Virtuozzo, do the following:

. Make sure that the following modules are added to the `IPTABLES_MODULES` variable in the /etc/sysconfig/iptables-config configuration file:
+
[subs="quotes"]
----
IPTABLES_MODULES="ip_conntrack ip_conntrack_ftp ip_conntrack_irc \
ip_conntrack_netbios_ns ip_nat_ftp ip_nat_irc ipt_comment ipt_conntrack \
ipt_helper ipt_length ipt_limit ipt_LOG ipt_multiport ipt_REDIRECT \
ipt_REJECT ipt_state ipt_tcp ipt_TCPMSS ipt_tcpmss ipt_tos ipt_TOS ipt_ttl \
iptable_filter iptable_mangle iptable_nat"
----
+
To load the modules on Virtuozzo, add them to the `IPTABLES_MODULES` variable in the `/etc/sysconfig/iptables-config` configuration file and restart the `iptables` service.

. Set the `ip_conntrack_disable_ve0` parameter to `0` in the `/etc/modprobe.d/parallels.conf` file.
. Restart the `iptables` service to apply changes:
+
[subs="quotes"]
----
*# service iptables restart*
----
. If required, you can check that the conntrack module is enabled with these commands:
+
[subs="quotes"]
----
*# cat /proc/net/ip_tables_names*
nat
*# iptables -t nat -L*
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
----

Limiting the Maximum conntrack Slots for Virtuozzo
++++++++++++++++++++++++++++++++++++++++++++++++++

To limit the maximum number of conntrack slots allowed on the Hardware Node, set the `net.nf_conntrack_max` variable. For example:

[subs="quotes"]
----
*# sysctl -w net.nf_conntrack_max=500000*
----

The value of `net.nf_conntrack_max` also restricts the value of `net.netfilter.nf_conntrack_max` which limits the maximum conntrack slots for each container on the Hardware Node.

Using iptables Modules in Containers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Using `iptables` modules in containers requires additional configuration on your part.

Configuring iptables Modules
++++++++++++++++++++++++++++

To set the state of `iptables` modules for backup/restore or live migration, use the `vzctl --netfilter` command. If some of the `iptables` modules allowed for a container are not loaded on the Hardware Node where that container has been restored or migrated, they will be automatically loaded when that container starts. For example, the command

[subs="quotes"]
----
*# vzctl set MyCT1 --netfilter stateful --save*
----

will make sure that all modules except NAT-related will be allowed and loaded for container MyCT1 (if required) on a Hardware Node where it has been restored or migrated.

[NOTE]
[subs="quotes"]
====
*Notes:*
[options="compact"]
. The default setting is `stateless`, which allows all modules except `conntrack` and NAT-related.
. To run Docker inside a Virtuozzo container, allow all `iptables` modules for that container with the `--netfilter full` command. For details on configuring Docker in Virtuozzo, see <<_creating_and_configuring_docker_enabled_containers>>.
====

Using conntrack Rules and NAT Tables
++++++++++++++++++++++++++++++++++++

By default, the NAT table and `conntrack` rules are disabled and not allowed for use in containers even if they are loaded on the server. To allow their use in containers, run the `vzctl set --netfilter full` command. For example, for container MyCT1:

[subs="quotes"]
----
*# vzctl set MyCT1 --netfilter full --save*
----

To limit the maximum number of conntrack slots available for each container on the Hardware Node, set the `net.netfilter.nf_conntrack_max` variable. For example:

[subs="quotes"]
----
*# sysctl -w net.netfilter.nf_conntrack_max=50000*
----

The value of `net.netfilter.nf_conntrack_max` cannot exceed the value of `net.nf_conntrack_max` (see <<_using_iptables_modules_in_virtuozzo>>).

[NOTE]
[subs="quotes"]
====
*Note:* Even if a container is under a DDoS attack and all its conntrack slots are in use, other containers will not be affected, still being able to create as many connections as set in `net.netfilter.nf_conntrack_max`.
====

Creating Configuration Files for New Linux Distributions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Distribution configuration files are used to distinguish among containers running different Linux versions and to determine what scripts should be executed when performing the relevant container-related operations (e.g., assigning a new IP address to the container).

All Linux distributions shipped with Virtuozzo have their own configuration files located in the `/etc/vz/conf/dists` directory on the hardware node. However, you may wish to create your own distribution configuration files to support new Linux versions released. Let us assume that you wish your containers to run the CentOS 7 Linux distribution and, therefore, have to make the `centos-7.conf` distribution configuration file to define what scripts are to be executed while performing major tasks with containers running this Linux version. To do this:

. In the container configuration file (with the name of +/etc/vz/conf/_&lt;UUID&gt;_.conf+), specify `centos-7` as the value of the `DISTRIBUTION` variable (for example, `DISTRIBUTION="centos-7"`).
. Create the `centos-7.conf` configuration file in the `/etc/vz/conf/dists` directory. The easiest way to do it is copy one of the existing configuration files by executing the following command in the `/etc/vz/conf/dists` directory:
+
[subs="quotes"]
----
*# cp fedora.conf centos-7.config*
----
+
In the example above, we assume that the `fedora.conf` file is present in the `/etc/vz/conf/dists` directory on the hardware node. In case it is not, you may use any other distribution configuration file available on your server.

. Open the `centos.conf` file for editing, go to the first entry and, in the right part of the entry, specify the name of the script you wish to be run on issuing the `prlctl` command with the parameter specified in the left part of the entry. For example, if you wish the script to be executed while assigning a new IP address to your container and the script has the `my_centos_script` name, your entry should look as follows:
+
----
ADD_IP=my_centos_script-add_ip.sh
----
. Repeat *Step 3* for all entries in the file.
. Place the scripts for the new Linux distribution to the `/etc/vz/conf/dists/scripts` directory on the Node. Make sure the names of these scripts coincide with those specified in the `centos-7.conf` file.

Installing Optional Virtuozzo Packages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Virtuozzo comes with everything you may need already installed. However, you can also install optional Virtuozzo packages from remote Odin repositories by means of the `yum` command.

[NOTE]
[subs="quotes"]
====
*Note:* For more information on using `yum` in Virtuozzo, see <<_updating_virtuozzo>> and the `yum` manual page.
====

Troubleshooting
---------------

This chapter provides the information about those problems that may occur during your work with Virtuozzo and suggests the ways to solve them, including getting technical support from Odin.

General Considerations
~~~~~~~~~~~~~~~~~~~~~~

The general issues to take into consideration when troubleshooting your system are listed below. You should read them carefully before trying to solve more specific problems.

* You should always remember where you are currently located in your terminal. Check it periodically using the `pwd`, `hostname`, `ifconfig`, `cat /proc/vz/veinfo` commands. One and the same command executed inside a container and on the Hardware Node can lead to very different results. You can also set up the `PS1` environment variable to show the full path in the `bash` prompt. To do this, add these lines to `/root/.bash_profile`:
+
----
PS1="[\u@\h \w]$ "
export PS1
----
* If the Hardware Node slows down, use `vmstat`, `ps` (`ps axfw`), `dmesg`, `top` (`vztop`) to find out what is happening, never reboot the machine without investigation. If no thinking helps restore the normal operation, use the Alt+SysRq sequences to dump the memory (`showMem`) and processes (`showPc`).
* Do not run any binary or script that belongs to a container directly from the Hardware Node, for example, do not ever do this:
+
[subs="quotes"]
----
*cd /vz/root/99/etc/init.d*
*./httpd status*
----
+
Any script inside a container could have been changed to whatever the container owner chooses: it could have been trojaned, replaced to something like `rm -rf`, etc. You can use only `prlctl exec/prlctl enter` to execute programs inside a container.

* Do not use init scripts on the Hardware Node. An init script may use `killall` to stop a service, which means that all similar processes will be killed in all containers. You can check `/var/run/Service.pid` and kill the correspondent process explicitly.
* You must be able to detect any rootkit inside a container. It is recommended to use the `chkrootkit` package for detection (you can download the latest version from http://www.chkrootkit.org), or at least run
+
[subs="quotes"]
----
*rpm -Va|grep "S.5"*
----
+
to check up if the MD5 sum has changed for any RPM file.
+
You can also run `nmap`, for example:
+
[subs="quotes"]
----
*# nmap -p 1-65535 192.168.0.1*
Starting nmap V. 2.54BETA22 ( www.insecure.org/nmap/ )
Interesting ports on  (192.168.0.1):
(The 65531 ports scanned but not shown below are in 
  state: closed)
Port       State       Service
21/tcp     open        ftp
22/tcp     open        ssh
80/tcp     open        http
111/tcp    open        sunrpc
Nmap run completed -- 1 IP address (1 host up) scanned 
  in 169 seconds
----
+
to check if any ports are open that should normally be closed.
+
That could however be a problem to remove a rootkit from a container and make sure it is 100% removed. If you're not sure, create a new container for that customer and migrate his/her sites and mail there.

* Check the `/var/log/` directory on the Hardware Node to find out what is happening on the system. There are a number of log files that are maintained by the system and Virtuozzo (the `boot.log`, `messages`, etc.), but other services and programs may also put their own log files here depending on your distribution of Linux and the services and applications that you are running. For example, there may be logs associated with running a mail server (the `maillog` file), automatic tasks (the `cron` file), and others. However, the first place to look into when you are troubleshooting is the `/var/log/messages` log file. It contains the boot messages when the system came up as well as other status messages as the system runs. Errors with I/O, networking, and other general system errors are reported in this file. So, we recommend that you read to the `messages` log file first and then proceed with the other files from the `/var/log/` directory.
* Subscribe to bug tracking lists. You should keep track of new public DoS tools or remote exploits for the software and install them into containers or at Hardware Nodes.
* When using `iptables`, there is a simple rule for Chains usage to help protect both the Hardware Node and its containers:
+
--
[options="compact"]
* use INPUT, OUTPUT to filter packets that come in/out the Hardware Node
* use FORWARD to filter packets that are designated for containers
--

Kernel Troubleshooting
~~~~~~~~~~~~~~~~~~~~~~

Using ALT+SYSRQ Keyboard Sequences
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Press ALT+SYSRQ+H and check what is printed at the Hardware Node console, for example:

----
SysRq: unRaw Boot Sync Unmount showPc showTasks showMem loglevel0-8 tErm kIll \
       killalL Calls Oops
----

This output shows you what ALT+SYSRQ sequences you may use for performing this or that command. The capital letters in the command names identify the sequence. Thus, if there are any troubles with the machine and you're about to reboot it, please use the following key sequences before pressing the *Power* button:

[options="compact"]
* ALT+SYSRQ+M to dump memory info
* ALT+SYSRQ+P to dump processes states
* ALT+SYSRQ+S to sync disks
* ALT+SYSRQ+U to unmount filesystems
* ALT+SYSRQ+L to kill all processes
* ALT+SYSRQ+U try to unmount once again
* ALT+SYSRQ+B to reboot

If the server is not rebooted after that, you can press the *Power* button.

Saving Kernel Faults (OOPS)
^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can use the following command to check for the kernel messages that should be reported to Virtuozzo developers:

[subs="quotes"]
----
*grep -E "Call Trace|Code" /var/log/messages**
----

Then, you should find kernel-related lines in the corresponding log file and figure out what kernel was booted when the oops occurred. Search backward for the `Linux` string, look for strings like:

----
Sep 26 11:41:12 kernel: Linux version 2.6.18-8.1.1.el5.028stab043.1 \
(root@rhel5-32-build) (gcc version 4.1.1 20061011 (Red Hat 4.1.1-30)) \
#1 SMP Wed Aug 29 11:51:58 MSK 2007
----

An oops usually starts with some description of what happened and ends with the Code string. Here is an example:

----
Aug 25 08:27:46 boar BUG: unable to handle kernel NULL pointer dereference at \
virtual address 00000038
Aug 25 08:27:46 boar printing eip:
Aug 25 08:27:46 boar f0ce6507
Aug 25 08:27:46 boar *pde = 00003001
Aug 25 08:27:46 boar Oops: 0000 [#1]
Aug 25 08:27:46 boar SMP
Aug 25 08:27:46 boar last sysfs file:
Aug 25 08:27:46 boar Modules linked in: snapapi26(U) bridge(U) ip_vzredir(U) \
vzredir(U) vzcompat(U) vzrst(U) i
p_nat(U) vzcpt(U) ip_conntrack(U) nfnetlink(U) vzfs(U) vzlinkdev(U) vzethdev(U) \
vzevent(U) vzlist(U) vznet(U) vzstat(U) vzmo
n(U) xt_tcpudp(U) ip_vznetstat(U) vznetstat(U) iptable_mangle(U) iptable_filter(U) \
ip_tables(U) vztable(U) vzdquota(U) vzdev(U) autofs4(U) hidp(U) rfcomm(U) l2cap(U) \
bluetooth(U) sunrpc(U) ipv6(U) xt_length(U) ipt_ttl(U) xt_tcpmss(U) ipt_TCPMSS(U) \
xt_multiport(U) xt_limit(U) ipt_tos(U) ipt_REJECT(U) x_tables(U) video(U) sbs(U) \
i2c_ec(U) button(U) battery(U) asus_acpi(U) ac(U) lp(U) floppy(U) sg(U) pcspkr(U) \
i2c_piix4(U) e100(U) parport_pc(U) i2c_core(U) parport(U) cpqphp(U) eepro100(U) \
mii(U) serio_raw(U) ide_cd(U) cdrom(U) ahci(U) libata(U) dm_snapshot
(U) dm_zero(U) dm_mirror(U) dm_mod(U) megaraid(U) sym53c8xx(U) \
scsi_transport_spi(U) sd_mod(U) scsi_mod(U) ext3(U) jbd(U) ehci_hcd(U) ohci_hcd(U) \
uhci_hcd(U)
Aug 25 08:27:46 boar CPU: 1, VCPU: -1.1
Aug 25 08:27:46 boar EIP: 0060:[<f0ce6507>] Tainted: P VLI
Aug 25 08:27:46 boar EFLAGS: 00010246 (2.6.18-028stab043.1-ent #1)
Aug 25 08:27:46 boar EIP is at clone_endio+0x29/0xc6 [dm_mod]
Aug 25 08:27:46 boar eax: 00000010   ebx: 00000001   ecx: 00000000   edx: 00000000
Aug 25 08:27:46 boar esi: 00000000   edi: b6f52920   ebp: c1a8dbc0   esp: 0b483e38
Aug 25 08:27:46 boar ds: 007b   es: 007b   ss: 0068
Aug 25 08:27:46 boar Process swapper (pid: 0, veid: 0, ti=0b482000 task=05e3f2b0 \
task.ti=0b482000)
Aug 25 08:27:46 boar Stack: 0b52caa0 00000001 00000000 b6f52920 00000000f0ce64de \
00000000 02478825
Aug 25 08:27:46 boar 00000000 c18a8620 b6f52920 271e1a8c 024ca03800000000 00000000 \
00000000
Aug 25 08:27:46 boar 00000000 00000000 c18a3c00 00000202 c189e89400000006 00000000 \
05cb7200
Aug 25 08:27:46 boar Call Trace:
Aug 25 08:27:46 boar [<f0ce64de>] clone_endio+0x0/0xc6 [dm_mod]
Aug 25 08:27:46 boar [<02478825>] bio_endio+0x50/0x55
Aug 25 08:27:46 boar [<024ca038>] __end_that_request_first+0x185/0x47c
Aug 25 08:27:46 boar [<f0c711eb>] scsi_end_request+0x1a/0xa9 [scsi_mod]
Aug 25 08:27:46 boar [<02458f04>] mempool_free+0x5f/0x63
Aug 25 08:27:46 boar
Aug 25 08:27:46 boar [<f0c713c3>] scsi_io_completion+0x149/0x2f3 [scsi_mod]
Aug 25 08:27:46 boar [<f0c333b9>] sd_rw_intr+0x1f1/0x21b [sd_mod]
Aug 25 08:27:46 boar [<f0c6d3b9>] scsi_finish_command+0x73/0x77 [scsi_mod]
Aug 25 08:27:46 boar [<024cbfa2>] blk_done_softirq+0x4d/0x58
Aug 25 08:27:46 boar [<02426452>] __do_softirq+0x84/0x109
Aug 25 08:27:46 boar [<0242650d>] do_softirq+0x36/0x3a
Aug 25 08:27:46 boar [<024050b7>] do_IRQ+0xad/0xb6
Aug 25 08:27:46 boar [<024023fa>] default_idle+0x0/0x59
Aug 25 08:27:46 boar [<0240242b>] default_idle+0x31/0x59
Aug 25 08:27:46 boar [<024024b1>] cpu_idle+0x5e/0x74
Aug 25 08:27:46 boar =======================
Aug 25 08:27:46 boar Code: 5d c3 55 57 89 c7 56 89 ce 53 bb 01 00 00 00 83 ec 0c \
8b 68 3c 83 7f 20 00 8b 45 00 8b 00 89 44 24 04 8b 45 04 89 04 24 8b 40 04 <8b> \
40 28 89 44 24 08 0f 85 86 00 00 00 f6 47 10 01 75 0a 85 c9
Aug 25 08:27:46 boar EIP: [<f0ce6507>] clone_endio+0x29/0xc6 [dm_mod] \
SS:ESP0068:0b483e38
Aug 25 08:27:46 boar Kernel panic - not syncing: Fatal exception in interrupt
----

All you need is to put the oops into a file and then send this file as part of your problem report to the Odin support team.

Finding a Kernel Function That Caused the D Process State
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If there are too many processes in the D state and you can't find out what is happening, issue the following command:

[subs="quotes"]
----
*# objdump -Dr /boot/vmlinux-\`uname -r\` &gt;/tmp/kernel.dump*
----

and then get the process list:

[subs="quotes"]
----
*# ps axfwln*
  F UID   PID  PPID PRI NI  VSZ  RSS  WCHAN STAT TTY TIME COMMAND
100   0 20418 20417  17  0 2588  684      - R    ?   0:00 ps axfwln
100   0     1     0   8  0 1388  524 145186 S    ?   0:00 init
040   0  8670     1   9  0 1448  960 145186 S    ?   0:00 syslogd -m 0
040   0  8713     1  10  0 1616 1140 11ea02 S    ?   0:00 crond
----

Look for a number under the *WCHAN* column for the process in question. Then, open `/tmp/kernel.dump` in an editor, find that number in the first column and then scroll backward to the first function name, which can look like this:

----
"c011e910 <sys_nanosleep>:"
----

Then you can tell if the process "lives" or is blocked into the found function.

Container Management Issues
~~~~~~~~~~~~~~~~~~~~~~~~~~~

This section includes recommendations on how to solve certain container issues.

Failure to Start a Container
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

An attempt to start a container fails.

Solution 1
++++++++++

If there is a message on the system console: `IP address is already used`, issue the `cat /proc/vz/veinfo` command. The information about the container numeric identifier, container class, number of container's processes and container IP address shall be displayed for each running container. This shall also demonstrate that your container is up, i.e. it must be running without any IP address assigned. Set its IP address using the command:

[subs="quotes"]
----
*# prlctl set _&lt;CT_name&gt;_ --ipadd _&lt;IP_address&gt;_*
----

where _<CT_name>_ is the container name and _<IP_address>_ is the desired IP address.

Solution 2
++++++++++

Poor UBC parameters might prevent the container from starting. Try to validate the container configuration. See what configuration parameters have caused the error and set appropriate values using the `prlctl set` command.

Solution 3
++++++++++

The container might have used all its disk quota (disk space). Check the quota (see <<_managing_disk_quotas>>) and adjust its parameters if needed.

Solution 4
++++++++++

Run the `prlctl console` utility to log in and get access to the container console. The utility will provide container startup/shutdown output that may be used to pinpoint the problem. For example:

[subs="quotes"]
----
*# prlctl console MyCT1*
----

where MyCT1 is container name.

Failure to Access a Container from Network
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Solution 1
++++++++++

The IP address assigned to the container might be already in use in your network. Make sure it is not. The problem container address can be checked by issuing the following command:

[subs="quotes"]
----
*# grep IP_ADDRESS /etc/vz/conf/_&lt;UUID&gt;_.conf*
IP_ADDRESS="10.0.186.101"
----

The IP addresses of other containers, which are running, can be checked by running

[subs="quotes"]
----
*# cat /proc/vz/veinfo*
----

Solution 2
++++++++++

Make sure the routing to the container is properly configured. containers can use the default router for your network, or you may configure the Hardware Node as router for its containers.

Failure to Log In to a Container
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The container starts successfully, but you cannot log in.

Solution 1
++++++++++

You are trying to connect via SSH, but access is denied. Probably you have not set the password of the `root` user yet or there is no such user. In this case, use the `prlctl set --userpasswd` command. For example, for container MyCT1 you might issue the following command:

[subs="quotes"]
----
*# prlctl set MyCT1 --userpasswd root:secret*
----

Solution 2
++++++++++

Check forwarding settings by issuing the following command:

[subs="quotes"]
----
*# cat /proc/sys/ipv4/conf/venet0/forwarding*
----

If it is `0` then change it to `1` by issuing the following command:

[subs="quotes"]
----
*# echo 1 &gt; /proc/sys/ipv4/conf/venet0/forwarding*
----

Getting Technical Support
~~~~~~~~~~~~~~~~~~~~~~~~~

This section provides information on how to get technical support from Odin.

Preparing and Sending Questions to Technical Support
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In most cases, the support team must rely on the customer's observations and communications with the customer to diagnose and solve the problem. Therefore, the detailed problem report is extremely important. You can submit a support report at http://www.odin.com/hcap/support/request/. When describing the problem, please do mention the following:

[options="compact"]
* symptoms of the problem
* when the problem began including the circumstances of the failure
* any changes you made to your system
* other information that may be relevant to your situation (e.g., the installation method)
* specific hardware devices that may be relevant to your problem

You can also use the Odin Support Request Tracker tool to submit a problem report:

[options="compact"]
. Visit https://support.odin.com.
. Enter the user name and password to log in to the Support Request Tracker. If you have not yet registered with the Support Request Tracker, click *Follow this link* and follow the instructions to get your login credentials.
. Log in to the Support Request Tracker using the received credentials.
. At the top of the displayed screen, select the name of your product and click the *New ticket in* button.
. Follow the on-screen instructions to create a new support ticket.

Another way of getting help is to directly call us or visit one of our offices. The contact information is available at http://www.odin.com/hcap/contact/.

Submitting Problem Reports to Technical Support
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can use the `prlctl problem-report` command to compile detailed reports for containers experiencing problems and send them to the Odin technical support. After receiving your report, the support team will closely examine your problem and make its best to solve it as quickly as possible.

[NOTE]
[subs="quotes"]
====
*Important:* Reports contain only logs and the information on your system and network settings. They do not contain any private information.
====

To generate a report, specify the UUID or name of the problem container and the way to process the report:

* To automatically send the report to the Odin technical support, pass the `-s` option to the command:
+
[subs="quotes"]
----
*# prlctl problem-report _&lt;CT_name&gt;_ -s*
----
+
This is the recommended way of running the command. If you use a proxy server to connect to the Internet, you need to additionally specify its parameters after the -`-proxy` option:
+
[subs="quotes"]
----
*# prlctl problem-report _&lt;CT_name&gt;_ -s --proxy \*
*[user[:password]@]&lt;proxyhost&gt;[:port]*
----
* To display the report on your screen in the machine-readable format, pass the `-d` option to the command. You can pipe the output to a file and then send it to the Odin technical support, for example:
+
[subs="quotes"]
----
*# prlctl problem-report _&lt;CT_name&gt;_ -d &gt; problemReport*
----
+
This command saves the generated report to the file `problemReport`.

Establishing a Secure Channel to Odin Support
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Virtuozzo provides you with a Virtuozzo Support Tunnel tool which allows you to establish a private secure channel to the Odin support team server. After establishing such a channel, the support team will be able to quickly and securely connect to your hardware node and diagnose and solve your problem. The secure connection to your server is achieved through a Virtual Private Network (VPN) created between the Odin support team server and your server.

To start using the Virtuozzo Support Tunnel tool:

* Make sure the `openvpn` (version 2.0 and above) and `vzvpn` packages are installed on your server. Both packages are available in the Virtuozzo remote repository, so you can install them using the `yum install` command:
+
[subs="quotes"]
----
*# yum install openvpn vzvpn*
----
* Make sure that port 80 is opened on the server.
* Edit the `/etc/vzvpn/vzvpn.conf` file to specify the correct parameters for your proxy server, if you use any.

After you have completed the tasks above and in case you encountered a problem, you can do the following to get assistance from the Odin support:

. Obtain a special certificate from Odin which will uniquely identify you as a Virtuozzo user. Certificates are issued by Odin in the form of files and should be installed on your server by issuing the `vzvpn.sh key-install certificate` command where `certificate` denotes the name of the certificate file obtained from Odin. You can get a certificate in one of the following ways:
** Visit http://www.odin.com/support/virtuozzo/certificates/, fill out the *Request Secure Virtuozzo Support Tunnel Certificate* form, and click *Submit* button. After a while, a certificate will be generated and sent to the email address you provided in the form.
** Contact the Odin support team via e-mail or by telephone and ask for a valid certificate.
. After you are ready with the certificate installation, make sure your server is connected to the Internet.
. On the server, execute the `/etc/init.d/vzvpn.sh start` command to establish a VPN between your server and the Odin support server.
. Contact the Odin support team (by telephone or via e-mail) and inform them of the problem you encountered. You should also mention that you have launched the Virtuozzo Support Tunnel tool and established a VPN to the Odin support server.
. After that, the Odin support team will connect to your server by using the secure VPN established, closely examine your problem, and make its best to solve the problem as quickly as possible.

[NOTE]
[subs="quotes"]
====
*Notes:*
[options="compact"]
. The Support Tunnel is implemented as a standard Linux service running in the background of your system. Therefore, to have this service running after your server reboot, you should set it to the `autoboot` mode or start it manually again by executing the `/etc/init.d/vzvpn start` command.
. To close the VPN session with the Odin support server, you should issue the `/etc/init.d/vzvpn stop` command on the server.
====

